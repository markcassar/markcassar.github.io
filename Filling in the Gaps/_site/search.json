[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Filling in the Gaps",
    "section": "",
    "text": "Understanding Byte Pair Encoding: Part 1: Encodings\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nMark Cassar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! This is where I will post Jupyter notebooks that I am using to understand various topics of interest. My hope is that they will help you too. s"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome/BPE_Part_1.html",
    "href": "posts/welcome/BPE_Part_1.html",
    "title": "Understanding Byte Pair Encoding: Part 1: Encodings",
    "section": "",
    "text": "My goal is to get a deeper understanding of tokenization as it relates to the preprocessing of text for input into a large language model (LLM). I had heard of byte pair encoding (BPE) but became more interested as I was reading Sebastian Raschka‚Äôs book Build a Large Language Model (from Scratch) (highly recommended). I then came across Andrej Karpathy‚Äôs incredible set of lectures From Zero to Hero, which includes a video on building the GPT2 tokenizer.\nWhat I am doing here is nothing original, just my attempt to process and understand as fully as possible the concepts I have been learning recently.\nSo, let‚Äôs start with some text:\n\ns = \"How are you?\"\nprint(s)\n\nHow are you?\n\n\nUnfortunately, text is never just ‚Äòtext‚Äô; there must be a mapping between binary numbers and characters, since all computers store information as binary numbers.\nWhen I look into the Python documentation, I discover that Python handles text as str (or string) objects; and further, that ‚ÄúStrings are immutable sequences of Unicode code points.‚Äù To access the Unicode code points, we can use the ord() function.\n\ns_list = list(s)\ncode_pts = [ord(ch) for ch in s]\n\nprint(s_list, end=\" \")\nprint()\nprint(code_pts, end=\" \")\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?'] \n[72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63] \n\n\nThe Unicode standard is an attempt to create a single system to represent all characters used in human communication. That is, to allow people to talk to each other how they naturally would regardless of the device they are using. This means the characters of all human languages, as well as things like emojis and mathematical symbols:\n\nord('Â•Ω'), ord('üòÄ'), ord('‚ü∫')\n\n(22909, 128512, 10234)\n\n\nThe standard consists of assigning a unique code point to each character. This is not an actual encoding, as it does not specify how this gets implemented on any computer hardware. Python displays these code points as decimal numbers, although the standard uses the notation U+ followed by a hexadecimal value.\n\nhex(ord('üòÄ')), hex(ord('‚ü∫'))\n\n('0x1f600', '0x27fa')\n\n\nSo the two characters above would have code points U+1F600 and U+27FA, respectively (‚Äò0x‚Äô is how Python designates a hexadecimal value).\nNow, since Unicode is not an encoding, my question was, how does Python know how to get from the characters to the code points? I think the answer is specified in the docs: ‚ÄúA Unicode string is a sequence of code points, which are numbers from 0 through 0x10FFFF (1,114,111 decimal). This sequence of code points needs to be represented in memory as a set of code units, and code units are then mapped to 8-bit bytes. The rules for translating a Unicode string into a sequence of bytes are called a character encoding, or just an encoding.‚Äù\nIt seems the answer is that Python assumes the text is encoded using UTF-8 and that the encoding is done natively under the hood. UTF stands for Unicode Transformation Format and the 8 stands for 8-bit.\nThe conversion between Unicode code points and UTF-8 is given the table from the Wikipedia page listed above:\n\n\n\nI need to keep a couple things in mind here: - UTF-8 operates at the single byte level, and - Unicode code points range from U+0000 to U+10FFFF, which in integers is from 0 to 1114111 (int(0x10ffff), noting that 0x10ffff is Python‚Äôs hexadecimal equivalent to U+10FFFF)\nGiven the range of code points, this means that UTF-8 needs more than one byte to represent any code point above (theoretically) 255; However, UTF-8 uses more than one byte to represent any code point above 127. This is done to maintain backward compatibility with ASCII, which only used values from 0 to 127.\nTo see this in action, I‚Äôll modify the text so it includes characters that require 1, 2, 3, and 4 bytes in UTF-8:\n\ns = \"How are you? œ± ‡§ú üòÄ\"\nprint(s)\n\nHow are you? œ± ‡§ú üòÄ\n\n\nNow I want to see the code points for these characters, also noting that there are 18 characters in the text.\n\nunicode_code_points = [ord(ch) for ch in s]\nhex_values = [hex(pt) for pt in unicode_code_points]\n\nprint(f\"Num chars: {len(unicode_code_points)}, Code pts: {unicode_code_points}\")\nprint(f\"Num chars: {len(hex_values)}, Code pts in hex: {hex_values}\")\n\nNum chars: 18, Code pts: [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 1009, 32, 2332, 32, 128512]\nNum chars: 18, Code pts in hex: ['0x48', '0x6f', '0x77', '0x20', '0x61', '0x72', '0x65', '0x20', '0x79', '0x6f', '0x75', '0x3f', '0x20', '0x3f1', '0x20', '0x91c', '0x20', '0x1f600']\n\n\nFor the moment, I want to look at the code points that are above 127, which are 1009, 2332, and 128512. Looking up these code points will often require the hexadecimal equivalent: - 1009 = 0x3f1 = U+3F1 - 2332 = 0x91c = U+91C - 128512 = 0x1f600 = U+1F600\nTo clarify this, I will note the equivalence mathematically. First, I note again that the prefix ‚Äò0x‚Äô is Python‚Äôs way of denoting a hexadecimal number. Thus, the values after that define the actual number. Second, I need to remember that in hexadecimal, we need to use the first few characters of the alphabet to represent the numbers from 10 to 15, so A=10, B=11, C=12, D=13, E=14, and F=15. So, we have:\n\\[\n\\begin{align*}\n\\rm{0x3f1} &= 3 \\times 16^2 + 15 \\times 16^1 + 1 \\times 16^0 \\\\\n&= 3 \\times 256 + 15 \\times 16 + 1 \\times 1 \\\\\n&= 768 + 240 + 1 \\\\\n&= 1009\\\\\n\\\\\n\\rm{0x91c} &= 9 \\times 16^2 + 1 \\times 16^1 + 12 \\times 16^0 \\\\\n&= 9 \\times 256 + 1 \\times 16 + 12 \\times 1 \\\\\n&= 2304 + 16 + 12 \\\\\n&= 2332\\\\\n\\\\\n\\rm{0x1f600} &= 1 \\times 16^4 + 15 \\times 16^3 + 6 \\times 16^2 + 0 \\times 16^1 + 0 \\times 16^0 \\\\\n&= 1 \\times 65536 + 15 \\times 4096 + 6 \\times 256 + 0 + 0 \\\\\n&= 65536 + 61440 + 1536 \\\\\n&= 128512\n\\end{align*}\n\\]\nI can also check the correspondence between these code point values and the characters that printed using the following tables: - for 1009 see Greek and Coptic - for 2332 (use hex value U+91C) see Devanagari - for 128512 (use hex value U+1F600) see Emoji\nNow when I look at the same text encoded in UTF-8, I see:\n\ns_utf8 = s.encode(\"utf-8\")\n\nprint(len(s_utf8), s_utf8)\nprint(len(list(s_utf8)), list(s_utf8))\n\n24 b'How are you? \\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n24 [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI now have 24 bytes, instead of 18 characters, and notice that all of the code point values are below 256 (which is expected if every value comes from a single byte). What I find confusing is reconciling this with what I just did above. To make sense of it, I need to recall the code point to UTF-8 conversion table above, and bring in binary numbers.\nNote what happens when I try to convert the UTF-8 encoded bytes as if they were code points:\n\nprint([chr(i) for i in list(s_utf8)])\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', '√è', '¬±', ' ', '√†', '¬§', '\\x9c', ' ', '√∞', '\\x9f', '\\x98', '\\x80']\n\n\nEverything is ok up until we hit the Greek letter rho, œ±. The problem here is that chr() is the reverse of ord() and so it only operates properly on code points, not UTF-8 bytes. To see this, we can use our earlier code point values:\n\nchr(1009), chr(2332), chr(128512)\n\n('œ±', '‡§ú', 'üòÄ')\n\n\nThat looks better! But how do I reconcile these two approaches. I‚Äôll first isolate our ‚Äúproblem‚Äù characters.\n\ns_prob = \"œ± ‡§ú üòÄ\".encode(\"utf-8\")\nprint(s_prob)\nlist(s_prob)\n\nb'\\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n\n\n[207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI know that a space character, ‚Äù ‚Äú, is represented by code point 32, so it seems that we have: 1. œ± somehow equivalent to two code points 207, 177 2. ‡§ú somehow equivalent to three code points 224, 164, 156 3. üòÄsomehow equivalent to the four code points 240, 159, 152, 128\nFor item 1 we could try:\n\nchr(207) + chr(177) # concatenate the two characters for the two code points\n\n'√è¬±'\n\n\nor\n\nchr(207 + 177) # find the character for the combined value of the two code points\n\n'∆Ä'\n\n\nNeither of those give the correct output. To get this to work I need to follow the UTF-8 guidelines for converting to Unicode code points (see table above). So for the Greek letter, let‚Äôs take a look at the byte values in binary and use the table to convert. \n\nfor b in s_prob[:2]:\n    print(b, hex(b), bin(b))\n\n207 0xcf 0b11001111\n177 0xb1 0b10110001\n\n\nThe \\(\\rm{\\textcolor{red}{0b}}\\) is Python‚Äôs designation for binary digit, and it is left out of the conversion table. So, instead of 207 and 177, we can deal 11001111 and 10110001. Now we can follow the UTF-8 encoding. The \\(\\textcolor{blue}{110}\\) at the beginning of the first number is a code to indicate that the character requires two bytes (\\(\\textcolor{blue}{1110}\\) if it requires three bytes and \\(\\textcolor{blue}{11110}\\) if it requires four bytes). Any byte beginning with a \\(\\textcolor{blue}{10}\\) denotes to Python that it belongs to a sequence of either 2, 3, or 4 bytes, and that it is NOT the starting byte (the source of many UnicodeDecodeErrors). Using this system, I get the following for œ±:\n\\[\n\\begin{align*}\n207 \\,\\, 177 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{110}\\, 01111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 110001 \\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,207\\,177}&= 01111 \\,\\,\\, 110001 \\\\\n&= 01111110001  \\\\\n\\rm{decimal\\,code\\, point\\, for\\,207\\,177}&= 0 \\times 2^{10} + 1 \\times 2^9 + 1 \\times 2^8 + 1 \\times 2^7 + 1 \\times 2^6 + 1 \\times 2^5 + 1 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 \\\\\n&= 0 + 512 + 256 + 128 + 64 + 32 + 16 + 0 + 0 + 0 + 1 \\\\\n&= 1009\n\\end{align*}\n\\]\nAnd just to validate that calculation:\n\nprint(int(0b01111110001))\nprint(2**9 + 2**8 + 2**7 + 2**6 + 2**5 + 2** 4 + 2**0)\n\n1009\n1009\n\n\nThis approach, then, provides a mechanism from going from stored binary digits to Unicode code points.\nSince I am a skeptical person, I want to see if this works for my other two ‚Äúproblem‚Äù characters. This time, however, I will leave out the direct conversion to decimal values. For these two characters, I have the following byte values:\n\nfor b in s_prob[3:]:\n    if b==32: # space character\n        print(\"space\")\n    else:\n        print(b, hex(b), bin(b))\n\n224 0xe0 0b11100000\n164 0xa4 0b10100100\n156 0x9c 0b10011100\nspace\n240 0xf0 0b11110000\n159 0x9f 0b10011111\n152 0x98 0b10011000\n128 0x80 0b10000000\n\n\nFor ‡§ú, I get the following:\n\\[\n\\begin{align*}\n224 \\,\\, 164\\,\\,156 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{1110}\\, 0000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 100100 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011100\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,224 \\,\\ 164\\,\\,156}&= 0000 \\,\\,\\, 100100 \\,\\,\\, 011100 \\\\\n&= 0000100100011100  \\\\\n\\end{align*}\n\\]\nwhich equals\n\nint(0b0000100100011100)\n\n2332\n\n\nAnd for üòÄ, I get:\n\\[\n\\begin{align*}\n240\\,\\,159 \\,\\, 152\\,\\,128 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{11110}\\, 000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\,000000\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,240\\,\\,159 \\,\\, 152\\,\\,128}&= 000 \\,\\,\\, 011111 \\,\\,\\, 011000 \\,\\,\\, 000000 \\\\\n&= 000011111011000000000  \\\\\n\\end{align*}\n\\]\nwhich is\n\nint(0b000011111011000000000)\n\n128512\n\n\nSo, I can see that everything is working as it is supposed to.\nBefore wrapping up this part of my journey, I want to mention that UTF-8 is not the only encoding scheme. UTF-16 and UTF-32 also exist. However, since UTF-8 seems to be the dominant encoding scheme at the moment, I won‚Äôt venture into the lands of UTF-16 and UTF-32.\nAnd the last point I want to make is that the 2, 3, and 4 byte sequences used in UTF-8 are linked bytes, so all is well if I execute\n\nb'How are you? \\xf0\\x9f\\x98\\x80'.decode(\"utf-8\")\n\n'How are you? üòÄ'\n\n\nBut not so good if I execute this\n\nb'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 b'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 13: invalid continuation byte\n\n\n\nThis is because the inserted space character is not a valid continuation byte according the the UTF-8 scheme.\nWell, that is it for the first part of my journey to dig into the details of BPE. Some aspects of encoding are still a bit murky, but I think I grasp enough at the moment to move on."
  },
  {
    "objectID": "posts/welcome/BPE_Part_1.html#understanding-byte-pair-encoding-part-1-encodings",
    "href": "posts/welcome/BPE_Part_1.html#understanding-byte-pair-encoding-part-1-encodings",
    "title": "Understanding Byte Pair Encoding: Part 1: Encodings",
    "section": "",
    "text": "My goal is to get a deeper understanding of tokenization as it relates to the preprocessing of text for input into a large language model (LLM). I had heard of byte pair encoding (BPE) but became more interested as I was reading Sebastian Raschka‚Äôs book Build a Large Language Model (from Scratch) (highly recommended). I then came across Andrej Karpathy‚Äôs incredible set of lectures From Zero to Hero, which includes a video on building the GPT2 tokenizer.\nWhat I am doing here is nothing original, just my attempt to process and understand as fully as possible the concepts I have been learning recently.\nSo, let‚Äôs start with some text:\n\ns = \"How are you?\"\nprint(s)\n\nHow are you?\n\n\nUnfortunately, text is never just ‚Äòtext‚Äô; there must be a mapping between binary numbers and characters, since all computers store information as binary numbers.\nWhen I look into the Python documentation, I discover that Python handles text as str (or string) objects; and further, that ‚ÄúStrings are immutable sequences of Unicode code points.‚Äù To access the Unicode code points, we can use the ord() function.\n\ns_list = list(s)\ncode_pts = [ord(ch) for ch in s]\n\nprint(s_list, end=\" \")\nprint()\nprint(code_pts, end=\" \")\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?'] \n[72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63] \n\n\nThe Unicode standard is an attempt to create a single system to represent all characters used in human communication. That is, to allow people to talk to each other how they naturally would regardless of the device they are using. This means the characters of all human languages, as well as things like emojis and mathematical symbols:\n\nord('Â•Ω'), ord('üòÄ'), ord('‚ü∫')\n\n(22909, 128512, 10234)\n\n\nThe standard consists of assigning a unique code point to each character. This is not an actual encoding, as it does not specify how this gets implemented on any computer hardware. Python displays these code points as decimal numbers, although the standard uses the notation U+ followed by a hexadecimal value.\n\nhex(ord('üòÄ')), hex(ord('‚ü∫'))\n\n('0x1f600', '0x27fa')\n\n\nSo the two characters above would have code points U+1F600 and U+27FA, respectively (‚Äò0x‚Äô is how Python designates a hexadecimal value).\nNow, since Unicode is not an encoding, my question was, how does Python know how to get from the characters to the code points? I think the answer is specified in the docs: ‚ÄúA Unicode string is a sequence of code points, which are numbers from 0 through 0x10FFFF (1,114,111 decimal). This sequence of code points needs to be represented in memory as a set of code units, and code units are then mapped to 8-bit bytes. The rules for translating a Unicode string into a sequence of bytes are called a character encoding, or just an encoding.‚Äù\nIt seems the answer is that Python assumes the text is encoded using UTF-8 and that the encoding is done natively under the hood. UTF stands for Unicode Transformation Format and the 8 stands for 8-bit.\nThe conversion between Unicode code points and UTF-8 is given the table from the Wikipedia page listed above:\n\n\n\nI need to keep a couple things in mind here: - UTF-8 operates at the single byte level, and - Unicode code points range from U+0000 to U+10FFFF, which in integers is from 0 to 1114111 (int(0x10ffff), noting that 0x10ffff is Python‚Äôs hexadecimal equivalent to U+10FFFF)\nGiven the range of code points, this means that UTF-8 needs more than one byte to represent any code point above (theoretically) 255; However, UTF-8 uses more than one byte to represent any code point above 127. This is done to maintain backward compatibility with ASCII, which only used values from 0 to 127.\nTo see this in action, I‚Äôll modify the text so it includes characters that require 1, 2, 3, and 4 bytes in UTF-8:\n\ns = \"How are you? œ± ‡§ú üòÄ\"\nprint(s)\n\nHow are you? œ± ‡§ú üòÄ\n\n\nNow I want to see the code points for these characters, also noting that there are 18 characters in the text.\n\nunicode_code_points = [ord(ch) for ch in s]\nhex_values = [hex(pt) for pt in unicode_code_points]\n\nprint(f\"Num chars: {len(unicode_code_points)}, Code pts: {unicode_code_points}\")\nprint(f\"Num chars: {len(hex_values)}, Code pts in hex: {hex_values}\")\n\nNum chars: 18, Code pts: [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 1009, 32, 2332, 32, 128512]\nNum chars: 18, Code pts in hex: ['0x48', '0x6f', '0x77', '0x20', '0x61', '0x72', '0x65', '0x20', '0x79', '0x6f', '0x75', '0x3f', '0x20', '0x3f1', '0x20', '0x91c', '0x20', '0x1f600']\n\n\nFor the moment, I want to look at the code points that are above 127, which are 1009, 2332, and 128512. Looking up these code points will often require the hexadecimal equivalent: - 1009 = 0x3f1 = U+3F1 - 2332 = 0x91c = U+91C - 128512 = 0x1f600 = U+1F600\nTo clarify this, I will note the equivalence mathematically. First, I note again that the prefix ‚Äò0x‚Äô is Python‚Äôs way of denoting a hexadecimal number. Thus, the values after that define the actual number. Second, I need to remember that in hexadecimal, we need to use the first few characters of the alphabet to represent the numbers from 10 to 15, so A=10, B=11, C=12, D=13, E=14, and F=15. So, we have:\n\\[\n\\begin{align*}\n\\rm{0x3f1} &= 3 \\times 16^2 + 15 \\times 16^1 + 1 \\times 16^0 \\\\\n&= 3 \\times 256 + 15 \\times 16 + 1 \\times 1 \\\\\n&= 768 + 240 + 1 \\\\\n&= 1009\\\\\n\\\\\n\\rm{0x91c} &= 9 \\times 16^2 + 1 \\times 16^1 + 12 \\times 16^0 \\\\\n&= 9 \\times 256 + 1 \\times 16 + 12 \\times 1 \\\\\n&= 2304 + 16 + 12 \\\\\n&= 2332\\\\\n\\\\\n\\rm{0x1f600} &= 1 \\times 16^4 + 15 \\times 16^3 + 6 \\times 16^2 + 0 \\times 16^1 + 0 \\times 16^0 \\\\\n&= 1 \\times 65536 + 15 \\times 4096 + 6 \\times 256 + 0 + 0 \\\\\n&= 65536 + 61440 + 1536 \\\\\n&= 128512\n\\end{align*}\n\\]\nI can also check the correspondence between these code point values and the characters that printed using the following tables: - for 1009 see Greek and Coptic - for 2332 (use hex value U+91C) see Devanagari - for 128512 (use hex value U+1F600) see Emoji\nNow when I look at the same text encoded in UTF-8, I see:\n\ns_utf8 = s.encode(\"utf-8\")\n\nprint(len(s_utf8), s_utf8)\nprint(len(list(s_utf8)), list(s_utf8))\n\n24 b'How are you? \\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n24 [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI now have 24 bytes, instead of 18 characters, and notice that all of the code point values are below 256 (which is expected if every value comes from a single byte). What I find confusing is reconciling this with what I just did above. To make sense of it, I need to recall the code point to UTF-8 conversion table above, and bring in binary numbers.\nNote what happens when I try to convert the UTF-8 encoded bytes as if they were code points:\n\nprint([chr(i) for i in list(s_utf8)])\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', '√è', '¬±', ' ', '√†', '¬§', '\\x9c', ' ', '√∞', '\\x9f', '\\x98', '\\x80']\n\n\nEverything is ok up until we hit the Greek letter rho, œ±. The problem here is that chr() is the reverse of ord() and so it only operates properly on code points, not UTF-8 bytes. To see this, we can use our earlier code point values:\n\nchr(1009), chr(2332), chr(128512)\n\n('œ±', '‡§ú', 'üòÄ')\n\n\nThat looks better! But how do I reconcile these two approaches. I‚Äôll first isolate our ‚Äúproblem‚Äù characters.\n\ns_prob = \"œ± ‡§ú üòÄ\".encode(\"utf-8\")\nprint(s_prob)\nlist(s_prob)\n\nb'\\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n\n\n[207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI know that a space character, ‚Äù ‚Äú, is represented by code point 32, so it seems that we have: 1. œ± somehow equivalent to two code points 207, 177 2. ‡§ú somehow equivalent to three code points 224, 164, 156 3. üòÄsomehow equivalent to the four code points 240, 159, 152, 128\nFor item 1 we could try:\n\nchr(207) + chr(177) # concatenate the two characters for the two code points\n\n'√è¬±'\n\n\nor\n\nchr(207 + 177) # find the character for the combined value of the two code points\n\n'∆Ä'\n\n\nNeither of those give the correct output. To get this to work I need to follow the UTF-8 guidelines for converting to Unicode code points (see table above). So for the Greek letter, let‚Äôs take a look at the byte values in binary and use the table to convert. \n\nfor b in s_prob[:2]:\n    print(b, hex(b), bin(b))\n\n207 0xcf 0b11001111\n177 0xb1 0b10110001\n\n\nThe \\(\\rm{\\textcolor{red}{0b}}\\) is Python‚Äôs designation for binary digit, and it is left out of the conversion table. So, instead of 207 and 177, we can deal 11001111 and 10110001. Now we can follow the UTF-8 encoding. The \\(\\textcolor{blue}{110}\\) at the beginning of the first number is a code to indicate that the character requires two bytes (\\(\\textcolor{blue}{1110}\\) if it requires three bytes and \\(\\textcolor{blue}{11110}\\) if it requires four bytes). Any byte beginning with a \\(\\textcolor{blue}{10}\\) denotes to Python that it belongs to a sequence of either 2, 3, or 4 bytes, and that it is NOT the starting byte (the source of many UnicodeDecodeErrors). Using this system, I get the following for œ±:\n\\[\n\\begin{align*}\n207 \\,\\, 177 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{110}\\, 01111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 110001 \\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,207\\,177}&= 01111 \\,\\,\\, 110001 \\\\\n&= 01111110001  \\\\\n\\rm{decimal\\,code\\, point\\, for\\,207\\,177}&= 0 \\times 2^{10} + 1 \\times 2^9 + 1 \\times 2^8 + 1 \\times 2^7 + 1 \\times 2^6 + 1 \\times 2^5 + 1 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 \\\\\n&= 0 + 512 + 256 + 128 + 64 + 32 + 16 + 0 + 0 + 0 + 1 \\\\\n&= 1009\n\\end{align*}\n\\]\nAnd just to validate that calculation:\n\nprint(int(0b01111110001))\nprint(2**9 + 2**8 + 2**7 + 2**6 + 2**5 + 2** 4 + 2**0)\n\n1009\n1009\n\n\nThis approach, then, provides a mechanism from going from stored binary digits to Unicode code points.\nSince I am a skeptical person, I want to see if this works for my other two ‚Äúproblem‚Äù characters. This time, however, I will leave out the direct conversion to decimal values. For these two characters, I have the following byte values:\n\nfor b in s_prob[3:]:\n    if b==32: # space character\n        print(\"space\")\n    else:\n        print(b, hex(b), bin(b))\n\n224 0xe0 0b11100000\n164 0xa4 0b10100100\n156 0x9c 0b10011100\nspace\n240 0xf0 0b11110000\n159 0x9f 0b10011111\n152 0x98 0b10011000\n128 0x80 0b10000000\n\n\nFor ‡§ú, I get the following:\n\\[\n\\begin{align*}\n224 \\,\\, 164\\,\\,156 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{1110}\\, 0000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 100100 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011100\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,224 \\,\\ 164\\,\\,156}&= 0000 \\,\\,\\, 100100 \\,\\,\\, 011100 \\\\\n&= 0000100100011100  \\\\\n\\end{align*}\n\\]\nwhich equals\n\nint(0b0000100100011100)\n\n2332\n\n\nAnd for üòÄ, I get:\n\\[\n\\begin{align*}\n240\\,\\,159 \\,\\, 152\\,\\,128 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{11110}\\, 000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\,000000\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,240\\,\\,159 \\,\\, 152\\,\\,128}&= 000 \\,\\,\\, 011111 \\,\\,\\, 011000 \\,\\,\\, 000000 \\\\\n&= 000011111011000000000  \\\\\n\\end{align*}\n\\]\nwhich is\n\nint(0b000011111011000000000)\n\n128512\n\n\nSo, I can see that everything is working as it is supposed to.\nBefore wrapping up this part of my journey, I want to mention that UTF-8 is not the only encoding scheme. UTF-16 and UTF-32 also exist. However, since UTF-8 seems to be the dominant encoding scheme at the moment, I won‚Äôt venture into the lands of UTF-16 and UTF-32.\nAnd the last point I want to make is that the 2, 3, and 4 byte sequences used in UTF-8 are linked bytes, so all is well if I execute\n\nb'How are you? \\xf0\\x9f\\x98\\x80'.decode(\"utf-8\")\n\n'How are you? üòÄ'\n\n\nBut not so good if I execute this\n\nb'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 b'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 13: invalid continuation byte\n\n\n\nThis is because the inserted space character is not a valid continuation byte according the the UTF-8 scheme.\nWell, that is it for the first part of my journey to dig into the details of BPE. Some aspects of encoding are still a bit murky, but I think I grasp enough at the moment to move on."
  },
  {
    "objectID": "posts/BPE_Part_1/BPE_Part_1.html",
    "href": "posts/BPE_Part_1/BPE_Part_1.html",
    "title": "Understanding Byte Pair Encoding: Part 1: Encodings",
    "section": "",
    "text": "My goal is to get a deeper understanding of tokenization as it relates to the preprocessing of text for input into a large language model (LLM). I had heard of byte pair encoding (BPE) but became more interested as I was reading Sebastian Raschka‚Äôs book Build a Large Language Model (from Scratch) (highly recommended). I then came across Andrej Karpathy‚Äôs incredible set of lectures From Zero to Hero, which includes a video on building the GPT2 tokenizer.\nWhat I am doing here is nothing original, just my attempt to process and understand as fully as possible the concepts I have been learning recently.\nSo, let‚Äôs start with some text:\n\ns = \"How are you?\"\nprint(s)\n\nHow are you?\n\n\nUnfortunately, text is never just ‚Äòtext‚Äô; there must be a mapping between binary numbers and characters, since all computers store information as binary numbers.\nWhen I look into the Python documentation, I discover that Python handles text as str (or string) objects; and further, that ‚ÄúStrings are immutable sequences of Unicode code points.‚Äù To access the Unicode code points, we can use the ord() function.\n\ns_list = list(s)\ncode_pts = [ord(ch) for ch in s]\n\nprint(s_list, end=\" \")\nprint()\nprint(code_pts, end=\" \")\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?'] \n[72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63] \n\n\nThe Unicode standard is an attempt to create a single system to represent all characters used in human communication. That is, to allow people to talk to each other how they naturally would regardless of the device they are using. This means the characters of all human languages, as well as things like emojis and mathematical symbols:\n\nord('Â•Ω'), ord('üòÄ'), ord('‚ü∫')\n\n(22909, 128512, 10234)\n\n\nThe standard consists of assigning a unique code point to each character. This is not an actual encoding, as it does not specify how this gets implemented on any computer hardware. Python displays these code points as decimal numbers, although the standard uses the notation U+ followed by a hexadecimal value.\n\nhex(ord('üòÄ')), hex(ord('‚ü∫'))\n\n('0x1f600', '0x27fa')\n\n\nSo the two characters above would have code points U+1F600 and U+27FA, respectively (‚Äò0x‚Äô is how Python designates a hexadecimal value).\nNow, since Unicode is not an encoding, my question was, how does Python know how to get from the characters to the code points? I think the answer is specified in the docs: ‚ÄúA Unicode string is a sequence of code points, which are numbers from 0 through 0x10FFFF (1,114,111 decimal). This sequence of code points needs to be represented in memory as a set of code units, and code units are then mapped to 8-bit bytes. The rules for translating a Unicode string into a sequence of bytes are called a character encoding, or just an encoding.‚Äù\nIt seems the answer is that Python assumes the text is encoded using UTF-8 and that the encoding is done natively under the hood. UTF stands for Unicode Transformation Format and the 8 stands for 8-bit.\nThe conversion between Unicode code points and UTF-8 is given the table from the Wikipedia page listed above:\n\n\n\nI need to keep a couple things in mind here: - UTF-8 operates at the single byte level, and - Unicode code points range from U+0000 to U+10FFFF, which in integers is from 0 to 1114111 (int(0x10ffff), noting that 0x10ffff is Python‚Äôs hexadecimal equivalent to U+10FFFF)\nGiven the range of code points, this means that UTF-8 needs more than one byte to represent any code point above (theoretically) 255; However, UTF-8 uses more than one byte to represent any code point above 127. This is done to maintain backward compatibility with ASCII, which only used values from 0 to 127.\nTo see this in action, I‚Äôll modify the text so it includes characters that require 1, 2, 3, and 4 bytes in UTF-8:\n\ns = \"How are you? œ± ‡§ú üòÄ\"\nprint(s)\n\nHow are you? œ± ‡§ú üòÄ\n\n\nNow I want to see the code points for these characters, also noting that there are 18 characters in the text.\n\nunicode_code_points = [ord(ch) for ch in s]\nhex_values = [hex(pt) for pt in unicode_code_points]\n\nprint(f\"Num chars: {len(unicode_code_points)}, Code pts: {unicode_code_points}\")\nprint(f\"Num chars: {len(hex_values)}, Code pts in hex: {hex_values}\")\n\nNum chars: 18, Code pts: [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 1009, 32, 2332, 32, 128512]\nNum chars: 18, Code pts in hex: ['0x48', '0x6f', '0x77', '0x20', '0x61', '0x72', '0x65', '0x20', '0x79', '0x6f', '0x75', '0x3f', '0x20', '0x3f1', '0x20', '0x91c', '0x20', '0x1f600']\n\n\nFor the moment, I want to look at the code points that are above 127, which are 1009, 2332, and 128512. Looking up these code points will often require the hexadecimal equivalent: - 1009 = 0x3f1 = U+3F1 - 2332 = 0x91c = U+91C - 128512 = 0x1f600 = U+1F600\nTo clarify this, I will note the equivalence mathematically. First, I note again that the prefix ‚Äò0x‚Äô is Python‚Äôs way of denoting a hexadecimal number. Thus, the values after that define the actual number. Second, I need to remember that in hexadecimal, we need to use the first few characters of the alphabet to represent the numbers from 10 to 15, so A=10, B=11, C=12, D=13, E=14, and F=15. So, we have:\n\\[\n\\begin{align*}\n\\rm{0x3f1} &= 3 \\times 16^2 + 15 \\times 16^1 + 1 \\times 16^0 \\\\\n&= 3 \\times 256 + 15 \\times 16 + 1 \\times 1 \\\\\n&= 768 + 240 + 1 \\\\\n&= 1009\\\\\n\\\\\n\\rm{0x91c} &= 9 \\times 16^2 + 1 \\times 16^1 + 12 \\times 16^0 \\\\\n&= 9 \\times 256 + 1 \\times 16 + 12 \\times 1 \\\\\n&= 2304 + 16 + 12 \\\\\n&= 2332\\\\\n\\\\\n\\rm{0x1f600} &= 1 \\times 16^4 + 15 \\times 16^3 + 6 \\times 16^2 + 0 \\times 16^1 + 0 \\times 16^0 \\\\\n&= 1 \\times 65536 + 15 \\times 4096 + 6 \\times 256 + 0 + 0 \\\\\n&= 65536 + 61440 + 1536 \\\\\n&= 128512\n\\end{align*}\n\\]\nI can also check the correspondence between these code point values and the characters that printed using the following tables: - for 1009 see Greek and Coptic - for 2332 (use hex value U+91C) see Devanagari - for 128512 (use hex value U+1F600) see Emoji\nNow when I look at the same text encoded in UTF-8, I see:\n\ns_utf8 = s.encode(\"utf-8\")\n\nprint(len(s_utf8), s_utf8)\nprint(len(list(s_utf8)), list(s_utf8))\n\n24 b'How are you? \\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n24 [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI now have 24 bytes, instead of 18 characters, and notice that all of the code point values are below 256 (which is expected if every value comes from a single byte). What I find confusing is reconciling this with what I just did above. To make sense of it, I need to recall the code point to UTF-8 conversion table above, and bring in binary numbers.\nNote what happens when I try to convert the UTF-8 encoded bytes as if they were code points:\n\nprint([chr(i) for i in list(s_utf8)])\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', '√è', '¬±', ' ', '√†', '¬§', '\\x9c', ' ', '√∞', '\\x9f', '\\x98', '\\x80']\n\n\nEverything is ok up until we hit the Greek letter rho, œ±. The problem here is that chr() is the reverse of ord() and so it only operates properly on code points, not UTF-8 bytes. To see this, we can use our earlier code point values:\n\nchr(1009), chr(2332), chr(128512)\n\n('œ±', '‡§ú', 'üòÄ')\n\n\nThat looks better! But how do I reconcile these two approaches. I‚Äôll first isolate our ‚Äúproblem‚Äù characters.\n\ns_prob = \"œ± ‡§ú üòÄ\".encode(\"utf-8\")\nprint(s_prob)\nlist(s_prob)\n\nb'\\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n\n\n[207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI know that a space character, ‚Äù ‚Äú, is represented by code point 32, so it seems that we have: 1. œ± somehow equivalent to two code points 207, 177 2. ‡§ú somehow equivalent to three code points 224, 164, 156 3. üòÄsomehow equivalent to the four code points 240, 159, 152, 128\nFor item 1 we could try:\n\nchr(207) + chr(177) # concatenate the two characters for the two code points\n\n'√è¬±'\n\n\nor\n\nchr(207 + 177) # find the character for the combined value of the two code points\n\n'∆Ä'\n\n\nNeither of those give the correct output. To get this to work I need to follow the UTF-8 guidelines for converting to Unicode code points (see table above). So for the Greek letter, let‚Äôs take a look at the byte values in binary and use the table to convert. \n\nfor b in s_prob[:2]:\n    print(b, hex(b), bin(b))\n\n207 0xcf 0b11001111\n177 0xb1 0b10110001\n\n\nThe \\(\\rm{\\textcolor{red}{0b}}\\) is Python‚Äôs designation for binary digit, and it is left out of the conversion table. So, instead of 207 and 177, we can deal 11001111 and 10110001. Now we can follow the UTF-8 encoding. The \\(\\textcolor{blue}{110}\\) at the beginning of the first number is a code to indicate that the character requires two bytes (\\(\\textcolor{blue}{1110}\\) if it requires three bytes and \\(\\textcolor{blue}{11110}\\) if it requires four bytes). Any byte beginning with a \\(\\textcolor{blue}{10}\\) denotes to Python that it belongs to a sequence of either 2, 3, or 4 bytes, and that it is NOT the starting byte (the source of many UnicodeDecodeErrors). Using this system, I get the following for œ±:\n\\[\n\\begin{align*}\n207 \\,\\, 177 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{110}\\, 01111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 110001 \\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,207\\,177}&= 01111 \\,\\,\\, 110001 \\\\\n&= 01111110001  \\\\\n\\rm{decimal\\,code\\, point\\, for\\,207\\,177}&= 0 \\times 2^{10} + 1 \\times 2^9 + 1 \\times 2^8 + 1 \\times 2^7 + 1 \\times 2^6 + 1 \\times 2^5 + 1 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 \\\\\n&= 0 + 512 + 256 + 128 + 64 + 32 + 16 + 0 + 0 + 0 + 1 \\\\\n&= 1009\n\\end{align*}\n\\]\nAnd just to validate that calculation:\n\nprint(int(0b01111110001))\nprint(2**9 + 2**8 + 2**7 + 2**6 + 2**5 + 2** 4 + 2**0)\n\n1009\n1009\n\n\nThis approach, then, provides a mechanism from going from stored binary digits to Unicode code points.\nSince I am a skeptical person, I want to see if this works for my other two ‚Äúproblem‚Äù characters. This time, however, I will leave out the direct conversion to decimal values. For these two characters, I have the following byte values:\n\nfor b in s_prob[3:]:\n    if b==32: # space character\n        print(\"space\")\n    else:\n        print(b, hex(b), bin(b))\n\n224 0xe0 0b11100000\n164 0xa4 0b10100100\n156 0x9c 0b10011100\nspace\n240 0xf0 0b11110000\n159 0x9f 0b10011111\n152 0x98 0b10011000\n128 0x80 0b10000000\n\n\nFor ‡§ú, I get the following:\n\\[\n\\begin{align*}\n224 \\,\\, 164\\,\\,156 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{1110}\\, 0000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 100100 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011100\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,224 \\,\\ 164\\,\\,156}&= 0000 \\,\\,\\, 100100 \\,\\,\\, 011100 \\\\\n&= 0000100100011100  \\\\\n\\end{align*}\n\\]\nwhich equals\n\nint(0b0000100100011100)\n\n2332\n\n\nAnd for üòÄ, I get:\n\\[\n\\begin{align*}\n240\\,\\,159 \\,\\, 152\\,\\,128 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{11110}\\, 000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\,000000\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,240\\,\\,159 \\,\\, 152\\,\\,128}&= 000 \\,\\,\\, 011111 \\,\\,\\, 011000 \\,\\,\\, 000000 \\\\\n&= 000011111011000000000  \\\\\n\\end{align*}\n\\]\nwhich is\n\nint(0b000011111011000000000)\n\n128512\n\n\nSo, I can see that everything is working as it is supposed to.\nBefore wrapping up this part of my journey, I want to mention that UTF-8 is not the only encoding scheme. UTF-16 and UTF-32 also exist. However, since UTF-8 seems to be the dominant encoding scheme at the moment, I won‚Äôt venture into the lands of UTF-16 and UTF-32.\nAnd the last point I want to make is that the 2, 3, and 4 byte sequences used in UTF-8 are linked bytes, so all is well if I execute\n\nb'How are you? \\xf0\\x9f\\x98\\x80'.decode(\"utf-8\")\n\n'How are you? üòÄ'\n\n\nBut not so good if I execute this\n\nb'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 b'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 13: invalid continuation byte\n\n\n\nThis is because the inserted space character is not a valid continuation byte according the the UTF-8 scheme.\nWell, that is it for the first part of my journey to dig into the details of BPE. Some aspects of encoding are still a bit murky, but I think I grasp enough at the moment to move on."
  },
  {
    "objectID": "posts/BPE_Part_1/BPE_Part_1.html#understanding-byte-pair-encoding-part-1-encodings",
    "href": "posts/BPE_Part_1/BPE_Part_1.html#understanding-byte-pair-encoding-part-1-encodings",
    "title": "Understanding Byte Pair Encoding: Part 1: Encodings",
    "section": "",
    "text": "My goal is to get a deeper understanding of tokenization as it relates to the preprocessing of text for input into a large language model (LLM). I had heard of byte pair encoding (BPE) but became more interested as I was reading Sebastian Raschka‚Äôs book Build a Large Language Model (from Scratch) (highly recommended). I then came across Andrej Karpathy‚Äôs incredible set of lectures From Zero to Hero, which includes a video on building the GPT2 tokenizer.\nWhat I am doing here is nothing original, just my attempt to process and understand as fully as possible the concepts I have been learning recently.\nSo, let‚Äôs start with some text:\n\ns = \"How are you?\"\nprint(s)\n\nHow are you?\n\n\nUnfortunately, text is never just ‚Äòtext‚Äô; there must be a mapping between binary numbers and characters, since all computers store information as binary numbers.\nWhen I look into the Python documentation, I discover that Python handles text as str (or string) objects; and further, that ‚ÄúStrings are immutable sequences of Unicode code points.‚Äù To access the Unicode code points, we can use the ord() function.\n\ns_list = list(s)\ncode_pts = [ord(ch) for ch in s]\n\nprint(s_list, end=\" \")\nprint()\nprint(code_pts, end=\" \")\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?'] \n[72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63] \n\n\nThe Unicode standard is an attempt to create a single system to represent all characters used in human communication. That is, to allow people to talk to each other how they naturally would regardless of the device they are using. This means the characters of all human languages, as well as things like emojis and mathematical symbols:\n\nord('Â•Ω'), ord('üòÄ'), ord('‚ü∫')\n\n(22909, 128512, 10234)\n\n\nThe standard consists of assigning a unique code point to each character. This is not an actual encoding, as it does not specify how this gets implemented on any computer hardware. Python displays these code points as decimal numbers, although the standard uses the notation U+ followed by a hexadecimal value.\n\nhex(ord('üòÄ')), hex(ord('‚ü∫'))\n\n('0x1f600', '0x27fa')\n\n\nSo the two characters above would have code points U+1F600 and U+27FA, respectively (‚Äò0x‚Äô is how Python designates a hexadecimal value).\nNow, since Unicode is not an encoding, my question was, how does Python know how to get from the characters to the code points? I think the answer is specified in the docs: ‚ÄúA Unicode string is a sequence of code points, which are numbers from 0 through 0x10FFFF (1,114,111 decimal). This sequence of code points needs to be represented in memory as a set of code units, and code units are then mapped to 8-bit bytes. The rules for translating a Unicode string into a sequence of bytes are called a character encoding, or just an encoding.‚Äù\nIt seems the answer is that Python assumes the text is encoded using UTF-8 and that the encoding is done natively under the hood. UTF stands for Unicode Transformation Format and the 8 stands for 8-bit.\nThe conversion between Unicode code points and UTF-8 is given the table from the Wikipedia page listed above:\n\n\n\nI need to keep a couple things in mind here: - UTF-8 operates at the single byte level, and - Unicode code points range from U+0000 to U+10FFFF, which in integers is from 0 to 1114111 (int(0x10ffff), noting that 0x10ffff is Python‚Äôs hexadecimal equivalent to U+10FFFF)\nGiven the range of code points, this means that UTF-8 needs more than one byte to represent any code point above (theoretically) 255; However, UTF-8 uses more than one byte to represent any code point above 127. This is done to maintain backward compatibility with ASCII, which only used values from 0 to 127.\nTo see this in action, I‚Äôll modify the text so it includes characters that require 1, 2, 3, and 4 bytes in UTF-8:\n\ns = \"How are you? œ± ‡§ú üòÄ\"\nprint(s)\n\nHow are you? œ± ‡§ú üòÄ\n\n\nNow I want to see the code points for these characters, also noting that there are 18 characters in the text.\n\nunicode_code_points = [ord(ch) for ch in s]\nhex_values = [hex(pt) for pt in unicode_code_points]\n\nprint(f\"Num chars: {len(unicode_code_points)}, Code pts: {unicode_code_points}\")\nprint(f\"Num chars: {len(hex_values)}, Code pts in hex: {hex_values}\")\n\nNum chars: 18, Code pts: [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 1009, 32, 2332, 32, 128512]\nNum chars: 18, Code pts in hex: ['0x48', '0x6f', '0x77', '0x20', '0x61', '0x72', '0x65', '0x20', '0x79', '0x6f', '0x75', '0x3f', '0x20', '0x3f1', '0x20', '0x91c', '0x20', '0x1f600']\n\n\nFor the moment, I want to look at the code points that are above 127, which are 1009, 2332, and 128512. Looking up these code points will often require the hexadecimal equivalent: - 1009 = 0x3f1 = U+3F1 - 2332 = 0x91c = U+91C - 128512 = 0x1f600 = U+1F600\nTo clarify this, I will note the equivalence mathematically. First, I note again that the prefix ‚Äò0x‚Äô is Python‚Äôs way of denoting a hexadecimal number. Thus, the values after that define the actual number. Second, I need to remember that in hexadecimal, we need to use the first few characters of the alphabet to represent the numbers from 10 to 15, so A=10, B=11, C=12, D=13, E=14, and F=15. So, we have:\n\\[\n\\begin{align*}\n\\rm{0x3f1} &= 3 \\times 16^2 + 15 \\times 16^1 + 1 \\times 16^0 \\\\\n&= 3 \\times 256 + 15 \\times 16 + 1 \\times 1 \\\\\n&= 768 + 240 + 1 \\\\\n&= 1009\\\\\n\\\\\n\\rm{0x91c} &= 9 \\times 16^2 + 1 \\times 16^1 + 12 \\times 16^0 \\\\\n&= 9 \\times 256 + 1 \\times 16 + 12 \\times 1 \\\\\n&= 2304 + 16 + 12 \\\\\n&= 2332\\\\\n\\\\\n\\rm{0x1f600} &= 1 \\times 16^4 + 15 \\times 16^3 + 6 \\times 16^2 + 0 \\times 16^1 + 0 \\times 16^0 \\\\\n&= 1 \\times 65536 + 15 \\times 4096 + 6 \\times 256 + 0 + 0 \\\\\n&= 65536 + 61440 + 1536 \\\\\n&= 128512\n\\end{align*}\n\\]\nI can also check the correspondence between these code point values and the characters that printed using the following tables: - for 1009 see Greek and Coptic - for 2332 (use hex value U+91C) see Devanagari - for 128512 (use hex value U+1F600) see Emoji\nNow when I look at the same text encoded in UTF-8, I see:\n\ns_utf8 = s.encode(\"utf-8\")\n\nprint(len(s_utf8), s_utf8)\nprint(len(list(s_utf8)), list(s_utf8))\n\n24 b'How are you? \\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n24 [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI now have 24 bytes, instead of 18 characters, and notice that all of the code point values are below 256 (which is expected if every value comes from a single byte). What I find confusing is reconciling this with what I just did above. To make sense of it, I need to recall the code point to UTF-8 conversion table above, and bring in binary numbers.\nNote what happens when I try to convert the UTF-8 encoded bytes as if they were code points:\n\nprint([chr(i) for i in list(s_utf8)])\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', '√è', '¬±', ' ', '√†', '¬§', '\\x9c', ' ', '√∞', '\\x9f', '\\x98', '\\x80']\n\n\nEverything is ok up until we hit the Greek letter rho, œ±. The problem here is that chr() is the reverse of ord() and so it only operates properly on code points, not UTF-8 bytes. To see this, we can use our earlier code point values:\n\nchr(1009), chr(2332), chr(128512)\n\n('œ±', '‡§ú', 'üòÄ')\n\n\nThat looks better! But how do I reconcile these two approaches. I‚Äôll first isolate our ‚Äúproblem‚Äù characters.\n\ns_prob = \"œ± ‡§ú üòÄ\".encode(\"utf-8\")\nprint(s_prob)\nlist(s_prob)\n\nb'\\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n\n\n[207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI know that a space character, ‚Äù ‚Äú, is represented by code point 32, so it seems that we have: 1. œ± somehow equivalent to two code points 207, 177 2. ‡§ú somehow equivalent to three code points 224, 164, 156 3. üòÄsomehow equivalent to the four code points 240, 159, 152, 128\nFor item 1 we could try:\n\nchr(207) + chr(177) # concatenate the two characters for the two code points\n\n'√è¬±'\n\n\nor\n\nchr(207 + 177) # find the character for the combined value of the two code points\n\n'∆Ä'\n\n\nNeither of those give the correct output. To get this to work I need to follow the UTF-8 guidelines for converting to Unicode code points (see table above). So for the Greek letter, let‚Äôs take a look at the byte values in binary and use the table to convert. \n\nfor b in s_prob[:2]:\n    print(b, hex(b), bin(b))\n\n207 0xcf 0b11001111\n177 0xb1 0b10110001\n\n\nThe \\(\\rm{\\textcolor{red}{0b}}\\) is Python‚Äôs designation for binary digit, and it is left out of the conversion table. So, instead of 207 and 177, we can deal 11001111 and 10110001. Now we can follow the UTF-8 encoding. The \\(\\textcolor{blue}{110}\\) at the beginning of the first number is a code to indicate that the character requires two bytes (\\(\\textcolor{blue}{1110}\\) if it requires three bytes and \\(\\textcolor{blue}{11110}\\) if it requires four bytes). Any byte beginning with a \\(\\textcolor{blue}{10}\\) denotes to Python that it belongs to a sequence of either 2, 3, or 4 bytes, and that it is NOT the starting byte (the source of many UnicodeDecodeErrors). Using this system, I get the following for œ±:\n\\[\n\\begin{align*}\n207 \\,\\, 177 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{110}\\, 01111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 110001 \\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,207\\,177}&= 01111 \\,\\,\\, 110001 \\\\\n&= 01111110001  \\\\\n\\rm{decimal\\,code\\, point\\, for\\,207\\,177}&= 0 \\times 2^{10} + 1 \\times 2^9 + 1 \\times 2^8 + 1 \\times 2^7 + 1 \\times 2^6 + 1 \\times 2^5 + 1 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 \\\\\n&= 0 + 512 + 256 + 128 + 64 + 32 + 16 + 0 + 0 + 0 + 1 \\\\\n&= 1009\n\\end{align*}\n\\]\nAnd just to validate that calculation:\n\nprint(int(0b01111110001))\nprint(2**9 + 2**8 + 2**7 + 2**6 + 2**5 + 2** 4 + 2**0)\n\n1009\n1009\n\n\nThis approach, then, provides a mechanism from going from stored binary digits to Unicode code points.\nSince I am a skeptical person, I want to see if this works for my other two ‚Äúproblem‚Äù characters. This time, however, I will leave out the direct conversion to decimal values. For these two characters, I have the following byte values:\n\nfor b in s_prob[3:]:\n    if b==32: # space character\n        print(\"space\")\n    else:\n        print(b, hex(b), bin(b))\n\n224 0xe0 0b11100000\n164 0xa4 0b10100100\n156 0x9c 0b10011100\nspace\n240 0xf0 0b11110000\n159 0x9f 0b10011111\n152 0x98 0b10011000\n128 0x80 0b10000000\n\n\nFor ‡§ú, I get the following:\n\\[\n\\begin{align*}\n224 \\,\\, 164\\,\\,156 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{1110}\\, 0000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 100100 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011100\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,224 \\,\\ 164\\,\\,156}&= 0000 \\,\\,\\, 100100 \\,\\,\\, 011100 \\\\\n&= 0000100100011100  \\\\\n\\end{align*}\n\\]\nwhich equals\n\nint(0b0000100100011100)\n\n2332\n\n\nAnd for üòÄ, I get:\n\\[\n\\begin{align*}\n240\\,\\,159 \\,\\, 152\\,\\,128 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{11110}\\, 000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\,000000\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,240\\,\\,159 \\,\\, 152\\,\\,128}&= 000 \\,\\,\\, 011111 \\,\\,\\, 011000 \\,\\,\\, 000000 \\\\\n&= 000011111011000000000  \\\\\n\\end{align*}\n\\]\nwhich is\n\nint(0b000011111011000000000)\n\n128512\n\n\nSo, I can see that everything is working as it is supposed to.\nBefore wrapping up this part of my journey, I want to mention that UTF-8 is not the only encoding scheme. UTF-16 and UTF-32 also exist. However, since UTF-8 seems to be the dominant encoding scheme at the moment, I won‚Äôt venture into the lands of UTF-16 and UTF-32.\nAnd the last point I want to make is that the 2, 3, and 4 byte sequences used in UTF-8 are linked bytes, so all is well if I execute\n\nb'How are you? \\xf0\\x9f\\x98\\x80'.decode(\"utf-8\")\n\n'How are you? üòÄ'\n\n\nBut not so good if I execute this\n\nb'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 b'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 13: invalid continuation byte\n\n\n\nThis is because the inserted space character is not a valid continuation byte according the the UTF-8 scheme.\nWell, that is it for the first part of my journey to dig into the details of BPE. Some aspects of encoding are still a bit murky, but I think I grasp enough at the moment to move on."
  },
  {
    "objectID": "posts/BPE_Part_1/png_jpeg.html",
    "href": "posts/BPE_Part_1/png_jpeg.html",
    "title": "Filling in the Gaps",
    "section": "",
    "text": "from PIL import Image \nfrom pathlib import Path\n\ncurdir = Path().cwd()\n\nfor f in curdir.iterdir():\n    if f.suffix.lower()=='.png':\n        img_png = Image.open(f)\n        # img_png = img_png.convert('RGB') # may or may not be needed\n        file = f.name[:-4] + '.jpeg'\n        img_png.save(curdir/file)"
  }
]