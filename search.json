[
  {
    "objectID": "posts/BPE_Part_3/BPE_Part_3.html",
    "href": "posts/BPE_Part_3/BPE_Part_3.html",
    "title": "Understanding Byte Pair Encoding: Part 3: the Algorithm",
    "section": "",
    "text": "I wrote about encodings and the basics of tokenization in my two earlier posts, so in this post, I will dig into the actual algorithm of byte-pair encoding (BPE). In the paper Language Models are Unsupervised Multitask Learners, which introduces GPT2, the authors note they use BPE at the byte level and that some preprocessing improves results by explicitly avoiding merges across character categories. It would seem, then, that interpreting text as a sequence of bytes, and not as a sequence of Unicode code points, is at the heart of the BPE method.\nI‚Äôll get into some of these preprocessing details in another post, but, for now, I just want to get an idea of how this works.\nThe paper that introduced BPE states that ‚ÄúThe algorithm compresses data by finding the most frequently occurring pairs of adjacent bytes in the data and replacing all instances of the pair with a byte that was not in the original data. The algorithm repeats this process until no further compression is possible, ‚Ä¶‚Äù\n\n\n\n\nCredit: Huggingface.co\n\n\n\n\n\nUsing the example text ababcabcd given in the paper noted above, I will outline the basic process. To do this, however, I need a way to find the most frequently occurring pairs:\n\ndef get_stats(chars):\n    stats = {}\n    for i in range(len(chars)-1):\n        stats[(chars[i], chars[i+1])] = stats.get((chars[i], chars[i+1]), 0) + 1 \n    stats = dict(sorted(stats.items(), key=lambda item: item[1], reverse=True))\n    return stats \n\nI will work at the character level for now, since it is easier to see how this works. Later, I will switch over to using bytes. To begin, I‚Äôll look at the number of unique tokens and note that no pairs have yet been merged:\n\ntext = 'ababcabcd' \ntokens = list(sorted(set(text)))\nmerges = {} \n\nprint(f\"Current text: {text}\")\nprint(f\"Number of characters in current text: {len(text)}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text: ababcabcd\nNumber of characters in current text: 9\nTokens: ['a', 'b', 'c', 'd']\nNumber of tokens: 4\nPairs merged: {}\n\n\nNow, I determine the frequency of occurrence for all the pairs of adjacent characters in the text:\n\nstats = get_stats(text)\nstats\n\n{('a', 'b'): 3, ('b', 'c'): 2, ('b', 'a'): 1, ('c', 'a'): 1, ('c', 'd'): 1}\n\n\nSince a and b occur together most frequently, we will merge those. To do this, we create a new character (one that doesn‚Äôt currently exist in our set of unique tokens), replace all occurrences of ab with the new character and keep track of the merge in the merges dictionary:\n\ntext_1 = text.replace('ab', 'X') \ntokens = list(sorted(set(text + text_1)))\nmerges[('a', 'b')] = 'X'\n\nprint(f\"Current text: {text_1}\")\nprint(f\"Number of characters in current text: {len(text_1)}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text: XXcXcd\nNumber of characters in current text: 6\nTokens: ['X', 'a', 'b', 'c', 'd']\nNumber of tokens: 5\nPairs merged: {('a', 'b'): 'X'}\n\n\nI now repeat the process:\n\nstats = get_stats(text_1)\nstats\n\n{('X', 'c'): 2, ('X', 'X'): 1, ('c', 'X'): 1, ('c', 'd'): 1}\n\n\nThis time, X and c are the most frequent pair.\n\ntext_2 = text_1.replace('Xc', 'Y') \ntokens = list(sorted(set(text + text_1 + text_2)))\nmerges[('X', 'c')] = 'Y'\n\n\nprint(f\"Current text: {text_2}\")\nprint(f\"Number of characters in current text: {len(text_2)}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text: XYYd\nNumber of characters in current text: 4\nTokens: ['X', 'Y', 'a', 'b', 'c', 'd']\nNumber of tokens: 6\nPairs merged: {('a', 'b'): 'X', ('X', 'c'): 'Y'}\n\n\nAnd again, I check the stats:\n\nstats = get_stats(text_2)\nstats\n\n{('X', 'Y'): 1, ('Y', 'Y'): 1, ('Y', 'd'): 1}\n\n\nNo further compression can happen now through the merging of pairs because if I merge, say, X and Y, since the pair only occurs once I achieve no compression because I also have to add a new token, say, R = XY, to my list of unique tokens. Thus, the number of characters in the text will go down by 1, but the number of tokens will go up by 1; there is no benefit to merging pairs that have a frequency of 1. So, I stop the process.\n\n\n\nFor clarity, at least to start, I kept this process at the character level. To get the method that was used for GPT2, I will repeat a slightly modified version of what I did above, but at the byte level. I discussed UTF-8 encoding in Part 1, but to make it easier to rember that we are working at the byte level, I have added some emojis to the original text:\n\ntext = 'üòÑüòÑ ababcabcd üòÑüòÑ'\nbyte_text = text.encode('utf-8')\n\nprint(f\"Text: {text}\")\nprint(f\"Number of characters in original text: {len(text)}\")\nprint()\nprint(f\"Raw bytes version of text: {byte_text}\")\nprint(f\"Number of bytes in raw bytes version of text: {len(byte_text)}\")\n\nText: üòÑüòÑ ababcabcd üòÑüòÑ\nNumber of characters in original text: 15\n\nRaw bytes version of text: b'\\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84 ababcabcd \\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84'\nNumber of bytes in raw bytes version of text: 27\n\n\nThe b in front of the string let‚Äôs me know that this is a Python bytes object:\n\ntype(byte_text)\n\nbytes\n\n\nThe UTF-8 encoding scheme allows any character to be represented by a sequence of 1, 2, 3, or 4 bytes. In the text here, the emoji requires 4 bytes but the lowercase English letters only need 1 byte each. I know each byte has a value between 0 and 255, so it will be easier to use the equivalent values instead of the raw bytes:\n\ntext_values = list(map(int, byte_text))\nprint(text_values)\n\n[240, 159, 152, 132, 240, 159, 152, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 240, 159, 152, 132, 240, 159, 152, 132]\n\n\nIt is important that I remember that the values here are not Unicode code points, they are byte values. The fact that Unicode characters with code points below 256 have the same value is simply a result of those characters can be represented by 1 byte and all such character code points equal to their byte value. To remember this, I look at the emoji, which is 4 bytes, and it‚Äôs Unicode code point is 128516. which is not equal to the ‚Äò240, 159, 152, 132‚Äô values as seen in text_values:\n\nord('a'), ord('üòÑ')\n\n(97, 128516)\n\n\nI will now repeat the process I did above:\n\ntokens = list(sorted(set(text_values)))\nmerges = {} \n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [240, 159, 152, 132, 240, 159, 152, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 240, 159, 152, 132, 240, 159, 152, 132]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240]\nPairs merged: {}\n\n\nAnd get the pair frequencies:\n\nstats = get_stats(text_values)\nstats\n\n{(240, 159): 4,\n (159, 152): 4,\n (152, 132): 4,\n (97, 98): 3,\n (132, 240): 2,\n (98, 99): 2,\n (132, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 240): 1}\n\n\nI have 3 pairs with the highest frequency, so I will pick the first that occurs, (240, 159), and merge those. Since I am dealing with byte values, instead of creating a new character not in my current set of tokens, I will create a new byte value for this merged pair. Since I am dealing in single bytes, the first available value will be 256.\nTo make this easier, I‚Äôll introduce a function to do the merging:\n\ndef replace_pairs(text, pair, idx):\n  new_text = []\n  i = 0\n  while i &lt; len(text):\n    if text[i] == pair[0] and i &lt; len(text) - 1 and text[i + 1] == pair[1]:\n      new_text.append(idx)\n      i += 2  \n    else:\n      new_text.append(text[i])\n      i += 1\n  return new_text\n\nAnd now I do the merge:\n\ntext_values = replace_pairs(text_values, (240, 159), 256)\ntokens.append(256)\nmerges[(240, 159)] = 256 \n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [256, 152, 132, 256, 152, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 256, 152, 132, 256, 152, 132]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256]\nPairs merged: {(240, 159): 256}\n\n\nCheck stats:\n\nstats = get_stats(text_values)\nstats\n\n{(256, 152): 4,\n (152, 132): 4,\n (97, 98): 3,\n (132, 256): 2,\n (98, 99): 2,\n (132, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 256): 1}\n\n\nMerge:\n\ntext_values = replace_pairs(text_values, (256, 152), 257)\ntokens.append(257)\nmerges[(256, 152)] = 257\n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [257, 132, 257, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 257, 132, 257, 132]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256, 257]\nPairs merged: {(240, 159): 256, (256, 152): 257}\n\n\nCheck stats:\n\nstats = get_stats(text_values)\nstats\n\n{(257, 132): 4,\n (97, 98): 3,\n (132, 257): 2,\n (98, 99): 2,\n (132, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 257): 1}\n\n\nMerge:\n\ntext_values = replace_pairs(text_values, (257, 132), 258)\ntokens.append(258)\nmerges[(257, 132)] = 258\n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [258, 258, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 258, 258]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256, 257, 258]\nPairs merged: {(240, 159): 256, (256, 152): 257, (257, 132): 258}\n\n\nCheck stats:\n\nstats = get_stats(text_values)\nstats\n\n{(97, 98): 3,\n (258, 258): 2,\n (98, 99): 2,\n (258, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 258): 1}\n\n\nMerge:\n\ntext_values = replace_pairs(text_values, (97, 98), 259)\ntokens.append(259)\nmerges[(97, 98)] = 259\n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [258, 258, 32, 259, 259, 99, 259, 99, 100, 32, 258, 258]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256, 257, 258, 259]\nPairs merged: {(240, 159): 256, (256, 152): 257, (257, 132): 258, (97, 98): 259}\n\n\nI will stop here, even though there are a couple more merges I could do, as I think the process is pretty clear now. All of this could, of course, be cleaned up code-wise but I wanted to go step by step with a semi-manual process so that the algorithm would sink in.\nWhat comes out of this BPE process is a set of tokens, which would be the vocabulary, and an ordered list of merges. With a sufficient amount of starting text, the vocabulary and merges would be sufficient to tokenize any text for input into a language model.\nThat‚Äôs it for now. In the next post I will go into some of the more nuanced modifications of BPE as done for GPT2."
  },
  {
    "objectID": "posts/BPE_Part_3/BPE_Part_3.html#understanding-byte-pair-encoding-part-3-the-algorithm",
    "href": "posts/BPE_Part_3/BPE_Part_3.html#understanding-byte-pair-encoding-part-3-the-algorithm",
    "title": "Understanding Byte Pair Encoding: Part 3: the Algorithm",
    "section": "",
    "text": "I wrote about encodings and the basics of tokenization in my two earlier posts, so in this post, I will dig into the actual algorithm of byte-pair encoding (BPE). In the paper Language Models are Unsupervised Multitask Learners, which introduces GPT2, the authors note they use BPE at the byte level and that some preprocessing improves results by explicitly avoiding merges across character categories. It would seem, then, that interpreting text as a sequence of bytes, and not as a sequence of Unicode code points, is at the heart of the BPE method.\nI‚Äôll get into some of these preprocessing details in another post, but, for now, I just want to get an idea of how this works.\nThe paper that introduced BPE states that ‚ÄúThe algorithm compresses data by finding the most frequently occurring pairs of adjacent bytes in the data and replacing all instances of the pair with a byte that was not in the original data. The algorithm repeats this process until no further compression is possible, ‚Ä¶‚Äù\n\n\n\n\nCredit: Huggingface.co\n\n\n\n\n\nUsing the example text ababcabcd given in the paper noted above, I will outline the basic process. To do this, however, I need a way to find the most frequently occurring pairs:\n\ndef get_stats(chars):\n    stats = {}\n    for i in range(len(chars)-1):\n        stats[(chars[i], chars[i+1])] = stats.get((chars[i], chars[i+1]), 0) + 1 \n    stats = dict(sorted(stats.items(), key=lambda item: item[1], reverse=True))\n    return stats \n\nI will work at the character level for now, since it is easier to see how this works. Later, I will switch over to using bytes. To begin, I‚Äôll look at the number of unique tokens and note that no pairs have yet been merged:\n\ntext = 'ababcabcd' \ntokens = list(sorted(set(text)))\nmerges = {} \n\nprint(f\"Current text: {text}\")\nprint(f\"Number of characters in current text: {len(text)}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text: ababcabcd\nNumber of characters in current text: 9\nTokens: ['a', 'b', 'c', 'd']\nNumber of tokens: 4\nPairs merged: {}\n\n\nNow, I determine the frequency of occurrence for all the pairs of adjacent characters in the text:\n\nstats = get_stats(text)\nstats\n\n{('a', 'b'): 3, ('b', 'c'): 2, ('b', 'a'): 1, ('c', 'a'): 1, ('c', 'd'): 1}\n\n\nSince a and b occur together most frequently, we will merge those. To do this, we create a new character (one that doesn‚Äôt currently exist in our set of unique tokens), replace all occurrences of ab with the new character and keep track of the merge in the merges dictionary:\n\ntext_1 = text.replace('ab', 'X') \ntokens = list(sorted(set(text + text_1)))\nmerges[('a', 'b')] = 'X'\n\nprint(f\"Current text: {text_1}\")\nprint(f\"Number of characters in current text: {len(text_1)}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text: XXcXcd\nNumber of characters in current text: 6\nTokens: ['X', 'a', 'b', 'c', 'd']\nNumber of tokens: 5\nPairs merged: {('a', 'b'): 'X'}\n\n\nI now repeat the process:\n\nstats = get_stats(text_1)\nstats\n\n{('X', 'c'): 2, ('X', 'X'): 1, ('c', 'X'): 1, ('c', 'd'): 1}\n\n\nThis time, X and c are the most frequent pair.\n\ntext_2 = text_1.replace('Xc', 'Y') \ntokens = list(sorted(set(text + text_1 + text_2)))\nmerges[('X', 'c')] = 'Y'\n\n\nprint(f\"Current text: {text_2}\")\nprint(f\"Number of characters in current text: {len(text_2)}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text: XYYd\nNumber of characters in current text: 4\nTokens: ['X', 'Y', 'a', 'b', 'c', 'd']\nNumber of tokens: 6\nPairs merged: {('a', 'b'): 'X', ('X', 'c'): 'Y'}\n\n\nAnd again, I check the stats:\n\nstats = get_stats(text_2)\nstats\n\n{('X', 'Y'): 1, ('Y', 'Y'): 1, ('Y', 'd'): 1}\n\n\nNo further compression can happen now through the merging of pairs because if I merge, say, X and Y, since the pair only occurs once I achieve no compression because I also have to add a new token, say, R = XY, to my list of unique tokens. Thus, the number of characters in the text will go down by 1, but the number of tokens will go up by 1; there is no benefit to merging pairs that have a frequency of 1. So, I stop the process.\n\n\n\nFor clarity, at least to start, I kept this process at the character level. To get the method that was used for GPT2, I will repeat a slightly modified version of what I did above, but at the byte level. I discussed UTF-8 encoding in Part 1, but to make it easier to rember that we are working at the byte level, I have added some emojis to the original text:\n\ntext = 'üòÑüòÑ ababcabcd üòÑüòÑ'\nbyte_text = text.encode('utf-8')\n\nprint(f\"Text: {text}\")\nprint(f\"Number of characters in original text: {len(text)}\")\nprint()\nprint(f\"Raw bytes version of text: {byte_text}\")\nprint(f\"Number of bytes in raw bytes version of text: {len(byte_text)}\")\n\nText: üòÑüòÑ ababcabcd üòÑüòÑ\nNumber of characters in original text: 15\n\nRaw bytes version of text: b'\\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84 ababcabcd \\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84'\nNumber of bytes in raw bytes version of text: 27\n\n\nThe b in front of the string let‚Äôs me know that this is a Python bytes object:\n\ntype(byte_text)\n\nbytes\n\n\nThe UTF-8 encoding scheme allows any character to be represented by a sequence of 1, 2, 3, or 4 bytes. In the text here, the emoji requires 4 bytes but the lowercase English letters only need 1 byte each. I know each byte has a value between 0 and 255, so it will be easier to use the equivalent values instead of the raw bytes:\n\ntext_values = list(map(int, byte_text))\nprint(text_values)\n\n[240, 159, 152, 132, 240, 159, 152, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 240, 159, 152, 132, 240, 159, 152, 132]\n\n\nIt is important that I remember that the values here are not Unicode code points, they are byte values. The fact that Unicode characters with code points below 256 have the same value is simply a result of those characters can be represented by 1 byte and all such character code points equal to their byte value. To remember this, I look at the emoji, which is 4 bytes, and it‚Äôs Unicode code point is 128516. which is not equal to the ‚Äò240, 159, 152, 132‚Äô values as seen in text_values:\n\nord('a'), ord('üòÑ')\n\n(97, 128516)\n\n\nI will now repeat the process I did above:\n\ntokens = list(sorted(set(text_values)))\nmerges = {} \n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [240, 159, 152, 132, 240, 159, 152, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 240, 159, 152, 132, 240, 159, 152, 132]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240]\nPairs merged: {}\n\n\nAnd get the pair frequencies:\n\nstats = get_stats(text_values)\nstats\n\n{(240, 159): 4,\n (159, 152): 4,\n (152, 132): 4,\n (97, 98): 3,\n (132, 240): 2,\n (98, 99): 2,\n (132, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 240): 1}\n\n\nI have 3 pairs with the highest frequency, so I will pick the first that occurs, (240, 159), and merge those. Since I am dealing with byte values, instead of creating a new character not in my current set of tokens, I will create a new byte value for this merged pair. Since I am dealing in single bytes, the first available value will be 256.\nTo make this easier, I‚Äôll introduce a function to do the merging:\n\ndef replace_pairs(text, pair, idx):\n  new_text = []\n  i = 0\n  while i &lt; len(text):\n    if text[i] == pair[0] and i &lt; len(text) - 1 and text[i + 1] == pair[1]:\n      new_text.append(idx)\n      i += 2  \n    else:\n      new_text.append(text[i])\n      i += 1\n  return new_text\n\nAnd now I do the merge:\n\ntext_values = replace_pairs(text_values, (240, 159), 256)\ntokens.append(256)\nmerges[(240, 159)] = 256 \n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [256, 152, 132, 256, 152, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 256, 152, 132, 256, 152, 132]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256]\nPairs merged: {(240, 159): 256}\n\n\nCheck stats:\n\nstats = get_stats(text_values)\nstats\n\n{(256, 152): 4,\n (152, 132): 4,\n (97, 98): 3,\n (132, 256): 2,\n (98, 99): 2,\n (132, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 256): 1}\n\n\nMerge:\n\ntext_values = replace_pairs(text_values, (256, 152), 257)\ntokens.append(257)\nmerges[(256, 152)] = 257\n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [257, 132, 257, 132, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 257, 132, 257, 132]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256, 257]\nPairs merged: {(240, 159): 256, (256, 152): 257}\n\n\nCheck stats:\n\nstats = get_stats(text_values)\nstats\n\n{(257, 132): 4,\n (97, 98): 3,\n (132, 257): 2,\n (98, 99): 2,\n (132, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 257): 1}\n\n\nMerge:\n\ntext_values = replace_pairs(text_values, (257, 132), 258)\ntokens.append(258)\nmerges[(257, 132)] = 258\n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [258, 258, 32, 97, 98, 97, 98, 99, 97, 98, 99, 100, 32, 258, 258]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256, 257, 258]\nPairs merged: {(240, 159): 256, (256, 152): 257, (257, 132): 258}\n\n\nCheck stats:\n\nstats = get_stats(text_values)\nstats\n\n{(97, 98): 3,\n (258, 258): 2,\n (98, 99): 2,\n (258, 32): 1,\n (32, 97): 1,\n (98, 97): 1,\n (99, 97): 1,\n (99, 100): 1,\n (100, 32): 1,\n (32, 258): 1}\n\n\nMerge:\n\ntext_values = replace_pairs(text_values, (97, 98), 259)\ntokens.append(259)\nmerges[(97, 98)] = 259\n\nprint(f\"Current text as byte values: {text_values}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Pairs merged: {merges}\")\n\nCurrent text as byte values: [258, 258, 32, 259, 259, 99, 259, 99, 100, 32, 258, 258]\nTokens: [32, 97, 98, 99, 100, 132, 152, 159, 240, 256, 257, 258, 259]\nPairs merged: {(240, 159): 256, (256, 152): 257, (257, 132): 258, (97, 98): 259}\n\n\nI will stop here, even though there are a couple more merges I could do, as I think the process is pretty clear now. All of this could, of course, be cleaned up code-wise but I wanted to go step by step with a semi-manual process so that the algorithm would sink in.\nWhat comes out of this BPE process is a set of tokens, which would be the vocabulary, and an ordered list of merges. With a sufficient amount of starting text, the vocabulary and merges would be sufficient to tokenize any text for input into a language model.\nThat‚Äôs it for now. In the next post I will go into some of the more nuanced modifications of BPE as done for GPT2."
  },
  {
    "objectID": "posts/BPE_Part_1/BPE_Part_1.html",
    "href": "posts/BPE_Part_1/BPE_Part_1.html",
    "title": "Understanding Byte Pair Encoding: Part 1: Encodings",
    "section": "",
    "text": "My goal is to get a deeper understanding of tokenization as it relates to the preprocessing of text for input into a large language model (LLM). I had heard of byte pair encoding (BPE) but became more interested as I was reading Sebastian Raschka‚Äôs book Build a Large Language Model (from Scratch) (highly recommended). I then came across Andrej Karpathy‚Äôs incredible set of lectures From Zero to Hero, which includes a video on building the GPT2 tokenizer.\nWhat I am doing here is nothing original, just my attempt to process and understand as fully as possible the concepts I have been learning recently.\nSo, let‚Äôs start with some text:\n\ns = \"How are you?\"\nprint(s)\n\nHow are you?\n\n\nUnfortunately, text is never just ‚Äòtext‚Äô; there must be a mapping between binary numbers and characters, since all computers store information as binary numbers.\nWhen I look into the Python documentation, I discover that Python handles text as str (or string) objects; and further, that ‚ÄúStrings are immutable sequences of Unicode code points.‚Äù To access the Unicode code points, we can use the ord() function.\n\ns_list = list(s)\ncode_pts = [ord(ch) for ch in s]\n\nprint(s_list, end=\" \")\nprint()\nprint(code_pts, end=\" \")\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?'] \n[72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63] \n\n\nThe Unicode standard is an attempt to create a single system to represent all characters used in human communication. That is, to allow people to talk to each other how they naturally would regardless of the device they are using. This means the characters of all human languages, as well as things like emojis and mathematical symbols:\n\nord('Â•Ω'), ord('üòÄ'), ord('‚ü∫')\n\n(22909, 128512, 10234)\n\n\nThe standard consists of assigning a unique code point to each character. This is not an actual encoding, as it does not specify how this gets implemented on any computer hardware. Python displays these code points as decimal numbers, although the standard uses the notation U+ followed by a hexadecimal value.\n\nhex(ord('üòÄ')), hex(ord('‚ü∫'))\n\n('0x1f600', '0x27fa')\n\n\nSo the two characters above would have code points U+1F600 and U+27FA, respectively (‚Äò0x‚Äô is how Python designates a hexadecimal value).\nNow, since Unicode is not an encoding, my question was, how does Python know how to get from the characters to the code points? I think the answer is specified in the docs: ‚ÄúA Unicode string is a sequence of code points, which are numbers from 0 through 0x10FFFF (1,114,111 decimal). This sequence of code points needs to be represented in memory as a set of code units, and code units are then mapped to 8-bit bytes. The rules for translating a Unicode string into a sequence of bytes are called a character encoding, or just an encoding.‚Äù\nIt seems the answer is that Python assumes the text is encoded using UTF-8 and that the encoding is done natively under the hood. UTF stands for Unicode Transformation Format and the 8 stands for 8-bit.\nThe conversion between Unicode code points and UTF-8 is given in the table from the Wikipedia page listed above:\n\n\n\nI need to keep a couple things in mind here: - UTF-8 operates at the single byte level, and - Unicode code points range from U+0000 to U+10FFFF, which in integers is from 0 to 1114111 (int(0x10ffff), noting that 0x10ffff is Python‚Äôs hexadecimal equivalent to U+10FFFF)\nGiven the range of code points, this means that UTF-8 needs more than one byte to represent any code point above (theoretically) 255; However, UTF-8 uses more than one byte to represent any code point above 127. This is done to maintain backward compatibility with ASCII, which only used values from 0 to 127.\nTo see this in action, I‚Äôll modify the text so it includes characters that require 1, 2, 3, and 4 bytes in UTF-8:\n\ns = \"How are you? œ± ‡§ú üòÄ\"\nprint(s)\n\nHow are you? œ± ‡§ú üòÄ\n\n\nNow I want to see the code points for these characters, also noting that there are 18 characters in the text.\n\nunicode_code_points = [ord(ch) for ch in s]\nhex_values = [hex(pt) for pt in unicode_code_points]\n\nprint(f\"Num chars: {len(unicode_code_points)}, Code pts: {unicode_code_points}\")\nprint(f\"Num chars: {len(hex_values)}, Code pts in hex: {hex_values}\")\n\nNum chars: 18, Code pts: [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 1009, 32, 2332, 32, 128512]\nNum chars: 18, Code pts in hex: ['0x48', '0x6f', '0x77', '0x20', '0x61', '0x72', '0x65', '0x20', '0x79', '0x6f', '0x75', '0x3f', '0x20', '0x3f1', '0x20', '0x91c', '0x20', '0x1f600']\n\n\nFor the moment, I want to look at the code points that are above 127, which are 1009, 2332, and 128512. Looking up these code points will often require the hexadecimal equivalent:\n\n1009 = 0x3f1 = U+3F1\n2332 = 0x91c = U+91C\n128512 = 0x1f600 = U+1F600\n\nTo clarify this, I will note the equivalence mathematically. First, I note again that the prefix ‚Äò0x‚Äô is Python‚Äôs way of denoting a hexadecimal number. Thus, the values after that define the actual number. Second, I need to remember that in hexadecimal, we need to use the first few characters of the alphabet to represent the numbers from 10 to 15, so A=10, B=11, C=12, D=13, E=14, and F=15. So, we have:\n\\[\n\\begin{align*}\n\\rm{0x3f1} &= 3 \\times 16^2 + 15 \\times 16^1 + 1 \\times 16^0 \\\\\n&= 3 \\times 256 + 15 \\times 16 + 1 \\times 1 \\\\\n&= 768 + 240 + 1 \\\\\n&= 1009\\\\\n\\\\\n\\rm{0x91c} &= 9 \\times 16^2 + 1 \\times 16^1 + 12 \\times 16^0 \\\\\n&= 9 \\times 256 + 1 \\times 16 + 12 \\times 1 \\\\\n&= 2304 + 16 + 12 \\\\\n&= 2332\\\\\n\\\\\n\\rm{0x1f600} &= 1 \\times 16^4 + 15 \\times 16^3 + 6 \\times 16^2 + 0 \\times 16^1 + 0 \\times 16^0 \\\\\n&= 1 \\times 65536 + 15 \\times 4096 + 6 \\times 256 + 0 + 0 \\\\\n&= 65536 + 61440 + 1536 \\\\\n&= 128512\n\\end{align*}\n\\]\nI can also check the correspondence between these code point values and the characters that printed using the following tables:\n\nfor 1009 see Greek and Coptic\nfor 2332 (use hex value U+91C) see Devanagari\nfor 128512 (use hex value U+1F600) see Emoji\n\nNow when I look at the same text encoded in UTF-8, I see:\n\ns_utf8 = s.encode(\"utf-8\")\n\nprint(len(s_utf8), s_utf8)\nprint(len(list(s_utf8)), list(s_utf8))\n\n24 b'How are you? \\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n24 [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI now have 24 bytes, instead of 18 characters, and notice that all of the code point values are below 256 (which is expected if every value comes from a single byte). What I find confusing is reconciling this with what I just did above. To make sense of it, I need to recall the code point to UTF-8 conversion table above, and bring in binary numbers.\nNote what happens when I try to convert the UTF-8 encoded bytes as if they were code points:\n\nprint([chr(i) for i in list(s_utf8)])\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', '√è', '¬±', ' ', '√†', '¬§', '\\x9c', ' ', '√∞', '\\x9f', '\\x98', '\\x80']\n\n\nEverything is ok up until we hit the Greek letter rho, œ±. The problem here is that chr() is the reverse of ord() and so it only operates properly on code points, not UTF-8 bytes. To see this, we can use our earlier code point values:\n\nchr(1009), chr(2332), chr(128512)\n\n('œ±', '‡§ú', 'üòÄ')\n\n\nThat looks better! But how do I reconcile these two approaches. I‚Äôll first isolate our ‚Äúproblem‚Äù characters.\n\ns_prob = \"œ± ‡§ú üòÄ\".encode(\"utf-8\")\nprint(s_prob)\nlist(s_prob)\n\nb'\\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n\n\n[207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI know that a space character, ‚Äù ‚Äú, is represented by code point 32, so it seems that we have:\n\nœ± somehow equivalent to two code points 207, 177\n‡§ú somehow equivalent to three code points 224, 164, 156\nüòÄsomehow equivalent to the four code points 240, 159, 152, 128\n\nFor item 1 we could try:\n\nchr(207) + chr(177) # concatenate the two characters for the two code points\n\n'√è¬±'\n\n\nor\n\nchr(207 + 177) # find the character for the combined value of the two code points\n\n'∆Ä'\n\n\nNeither of those give the correct output. To get this to work I need to follow the UTF-8 guidelines for converting to Unicode code points (see table above). So for the Greek letter, let‚Äôs take a look at the byte values in binary and use the table to convert. \n\nfor b in s_prob[:2]:\n    print(b, hex(b), bin(b))\n\n207 0xcf 0b11001111\n177 0xb1 0b10110001\n\n\nThe \\(\\rm{\\textcolor{red}{0b}}\\) is Python‚Äôs designation for binary digit, and it is left out of the conversion table. So, instead of 207 and 177, we can deal with 11001111 and 10110001. Now we can follow the UTF-8 encoding. The \\(\\textcolor{blue}{110}\\) at the beginning of the first number is a code to indicate that the character requires two bytes (\\(\\textcolor{blue}{1110}\\) if it requires three bytes and \\(\\textcolor{blue}{11110}\\) if it requires four bytes). Any byte beginning with a \\(\\textcolor{blue}{10}\\) denotes to Python that it belongs to a sequence of either 2, 3, or 4 bytes, and that it is NOT the starting byte (the source of many UnicodeDecodeErrors, see below). Using this system, I get the following for œ±:\n\\[\n\\begin{align*}\n207 \\,\\, 177 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{110}\\, 01111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 110001 \\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,207\\,177}&= 01111 \\,\\,\\, 110001 \\\\\n&= 01111110001  \\\\\n\\rm{decimal\\,code\\, point\\, for\\,207\\,177}&= 0 \\times 2^{10} + 1 \\times 2^9 + 1 \\times 2^8 + 1 \\times 2^7 + 1 \\times 2^6 + 1 \\times 2^5 + 1 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 \\\\\n&= 0 + 512 + 256 + 128 + 64 + 32 + 16 + 0 + 0 + 0 + 1 \\\\\n&= 1009\n\\end{align*}\n\\]\nAnd just to validate that calculation:\n\nprint(int(0b01111110001))\nprint(2**9 + 2**8 + 2**7 + 2**6 + 2**5 + 2** 4 + 2**0)\n\n1009\n1009\n\n\nThis approach, then, provides a mechanism from going from stored binary digits to Unicode code points.\nSince I am a skeptical person, I want to see if this works for my other two ‚Äúproblem‚Äù characters. This time, however, I will leave out the direct conversion to decimal values. For these two characters, I have the following byte values:\n\nfor b in s_prob[3:]:\n    if b==32: # space character\n        print(\"space\")\n    else:\n        print(b, hex(b), bin(b))\n\n224 0xe0 0b11100000\n164 0xa4 0b10100100\n156 0x9c 0b10011100\nspace\n240 0xf0 0b11110000\n159 0x9f 0b10011111\n152 0x98 0b10011000\n128 0x80 0b10000000\n\n\nFor ‡§ú, I get the following:\n\\[\n\\begin{align*}\n224 \\,\\, 164\\,\\,156 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{1110}\\, 0000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 100100 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011100\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,224 \\,\\ 164\\,\\,156}&= 0000 \\,\\,\\, 100100 \\,\\,\\, 011100 \\\\\n&= 0000100100011100  \\\\\n\\end{align*}\n\\]\nwhich equals\n\nint(0b0000100100011100)\n\n2332\n\n\nAnd for üòÄ, I get:\n\\[\n\\begin{align*}\n240\\,\\,159 \\,\\, 152\\,\\,128 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{11110}\\, 000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\,000000\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,240\\,\\,159 \\,\\, 152\\,\\,128}&= 000 \\,\\,\\, 011111 \\,\\,\\, 011000 \\,\\,\\, 000000 \\\\\n&= 000011111011000000000  \\\\\n\\end{align*}\n\\]\nwhich is\n\nint(0b000011111011000000000)\n\n128512\n\n\nSo, I can see that everything is working as it is supposed to.\nBefore wrapping up this part of my journey, I want to mention that UTF-8 is not the only encoding scheme. UTF-16 and UTF-32 also exist. However, since UTF-8 seems to be the dominant encoding scheme at the moment, I won‚Äôt venture into the lands of UTF-16 and UTF-32.\nAnd the last point I want to make is that the 2, 3, and 4 byte sequences used in UTF-8 are linked bytes, so all is well if I execute\n\nb'How are you? \\xf0\\x9f\\x98\\x80'.decode(\"utf-8\")\n\n'How are you? üòÄ'\n\n\nBut not so good if I execute this\n\nb'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 b'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 13: invalid continuation byte\n\n\n\nThis is because the inserted space character is not a valid continuation byte according the the UTF-8 scheme.\nWell, that is it for the first part of my journey to dig into the details of BPE. Some aspects of encoding are still a bit murky, but I think I grasp enough at the moment to move on.\nDownload this notebook"
  },
  {
    "objectID": "posts/BPE_Part_1/BPE_Part_1.html#understanding-byte-pair-encoding-part-1-encodings",
    "href": "posts/BPE_Part_1/BPE_Part_1.html#understanding-byte-pair-encoding-part-1-encodings",
    "title": "Understanding Byte Pair Encoding: Part 1: Encodings",
    "section": "",
    "text": "My goal is to get a deeper understanding of tokenization as it relates to the preprocessing of text for input into a large language model (LLM). I had heard of byte pair encoding (BPE) but became more interested as I was reading Sebastian Raschka‚Äôs book Build a Large Language Model (from Scratch) (highly recommended). I then came across Andrej Karpathy‚Äôs incredible set of lectures From Zero to Hero, which includes a video on building the GPT2 tokenizer.\nWhat I am doing here is nothing original, just my attempt to process and understand as fully as possible the concepts I have been learning recently.\nSo, let‚Äôs start with some text:\n\ns = \"How are you?\"\nprint(s)\n\nHow are you?\n\n\nUnfortunately, text is never just ‚Äòtext‚Äô; there must be a mapping between binary numbers and characters, since all computers store information as binary numbers.\nWhen I look into the Python documentation, I discover that Python handles text as str (or string) objects; and further, that ‚ÄúStrings are immutable sequences of Unicode code points.‚Äù To access the Unicode code points, we can use the ord() function.\n\ns_list = list(s)\ncode_pts = [ord(ch) for ch in s]\n\nprint(s_list, end=\" \")\nprint()\nprint(code_pts, end=\" \")\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?'] \n[72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63] \n\n\nThe Unicode standard is an attempt to create a single system to represent all characters used in human communication. That is, to allow people to talk to each other how they naturally would regardless of the device they are using. This means the characters of all human languages, as well as things like emojis and mathematical symbols:\n\nord('Â•Ω'), ord('üòÄ'), ord('‚ü∫')\n\n(22909, 128512, 10234)\n\n\nThe standard consists of assigning a unique code point to each character. This is not an actual encoding, as it does not specify how this gets implemented on any computer hardware. Python displays these code points as decimal numbers, although the standard uses the notation U+ followed by a hexadecimal value.\n\nhex(ord('üòÄ')), hex(ord('‚ü∫'))\n\n('0x1f600', '0x27fa')\n\n\nSo the two characters above would have code points U+1F600 and U+27FA, respectively (‚Äò0x‚Äô is how Python designates a hexadecimal value).\nNow, since Unicode is not an encoding, my question was, how does Python know how to get from the characters to the code points? I think the answer is specified in the docs: ‚ÄúA Unicode string is a sequence of code points, which are numbers from 0 through 0x10FFFF (1,114,111 decimal). This sequence of code points needs to be represented in memory as a set of code units, and code units are then mapped to 8-bit bytes. The rules for translating a Unicode string into a sequence of bytes are called a character encoding, or just an encoding.‚Äù\nIt seems the answer is that Python assumes the text is encoded using UTF-8 and that the encoding is done natively under the hood. UTF stands for Unicode Transformation Format and the 8 stands for 8-bit.\nThe conversion between Unicode code points and UTF-8 is given in the table from the Wikipedia page listed above:\n\n\n\nI need to keep a couple things in mind here: - UTF-8 operates at the single byte level, and - Unicode code points range from U+0000 to U+10FFFF, which in integers is from 0 to 1114111 (int(0x10ffff), noting that 0x10ffff is Python‚Äôs hexadecimal equivalent to U+10FFFF)\nGiven the range of code points, this means that UTF-8 needs more than one byte to represent any code point above (theoretically) 255; However, UTF-8 uses more than one byte to represent any code point above 127. This is done to maintain backward compatibility with ASCII, which only used values from 0 to 127.\nTo see this in action, I‚Äôll modify the text so it includes characters that require 1, 2, 3, and 4 bytes in UTF-8:\n\ns = \"How are you? œ± ‡§ú üòÄ\"\nprint(s)\n\nHow are you? œ± ‡§ú üòÄ\n\n\nNow I want to see the code points for these characters, also noting that there are 18 characters in the text.\n\nunicode_code_points = [ord(ch) for ch in s]\nhex_values = [hex(pt) for pt in unicode_code_points]\n\nprint(f\"Num chars: {len(unicode_code_points)}, Code pts: {unicode_code_points}\")\nprint(f\"Num chars: {len(hex_values)}, Code pts in hex: {hex_values}\")\n\nNum chars: 18, Code pts: [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 1009, 32, 2332, 32, 128512]\nNum chars: 18, Code pts in hex: ['0x48', '0x6f', '0x77', '0x20', '0x61', '0x72', '0x65', '0x20', '0x79', '0x6f', '0x75', '0x3f', '0x20', '0x3f1', '0x20', '0x91c', '0x20', '0x1f600']\n\n\nFor the moment, I want to look at the code points that are above 127, which are 1009, 2332, and 128512. Looking up these code points will often require the hexadecimal equivalent:\n\n1009 = 0x3f1 = U+3F1\n2332 = 0x91c = U+91C\n128512 = 0x1f600 = U+1F600\n\nTo clarify this, I will note the equivalence mathematically. First, I note again that the prefix ‚Äò0x‚Äô is Python‚Äôs way of denoting a hexadecimal number. Thus, the values after that define the actual number. Second, I need to remember that in hexadecimal, we need to use the first few characters of the alphabet to represent the numbers from 10 to 15, so A=10, B=11, C=12, D=13, E=14, and F=15. So, we have:\n\\[\n\\begin{align*}\n\\rm{0x3f1} &= 3 \\times 16^2 + 15 \\times 16^1 + 1 \\times 16^0 \\\\\n&= 3 \\times 256 + 15 \\times 16 + 1 \\times 1 \\\\\n&= 768 + 240 + 1 \\\\\n&= 1009\\\\\n\\\\\n\\rm{0x91c} &= 9 \\times 16^2 + 1 \\times 16^1 + 12 \\times 16^0 \\\\\n&= 9 \\times 256 + 1 \\times 16 + 12 \\times 1 \\\\\n&= 2304 + 16 + 12 \\\\\n&= 2332\\\\\n\\\\\n\\rm{0x1f600} &= 1 \\times 16^4 + 15 \\times 16^3 + 6 \\times 16^2 + 0 \\times 16^1 + 0 \\times 16^0 \\\\\n&= 1 \\times 65536 + 15 \\times 4096 + 6 \\times 256 + 0 + 0 \\\\\n&= 65536 + 61440 + 1536 \\\\\n&= 128512\n\\end{align*}\n\\]\nI can also check the correspondence between these code point values and the characters that printed using the following tables:\n\nfor 1009 see Greek and Coptic\nfor 2332 (use hex value U+91C) see Devanagari\nfor 128512 (use hex value U+1F600) see Emoji\n\nNow when I look at the same text encoded in UTF-8, I see:\n\ns_utf8 = s.encode(\"utf-8\")\n\nprint(len(s_utf8), s_utf8)\nprint(len(list(s_utf8)), list(s_utf8))\n\n24 b'How are you? \\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n24 [72, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI now have 24 bytes, instead of 18 characters, and notice that all of the code point values are below 256 (which is expected if every value comes from a single byte). What I find confusing is reconciling this with what I just did above. To make sense of it, I need to recall the code point to UTF-8 conversion table above, and bring in binary numbers.\nNote what happens when I try to convert the UTF-8 encoded bytes as if they were code points:\n\nprint([chr(i) for i in list(s_utf8)])\n\n['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', '√è', '¬±', ' ', '√†', '¬§', '\\x9c', ' ', '√∞', '\\x9f', '\\x98', '\\x80']\n\n\nEverything is ok up until we hit the Greek letter rho, œ±. The problem here is that chr() is the reverse of ord() and so it only operates properly on code points, not UTF-8 bytes. To see this, we can use our earlier code point values:\n\nchr(1009), chr(2332), chr(128512)\n\n('œ±', '‡§ú', 'üòÄ')\n\n\nThat looks better! But how do I reconcile these two approaches. I‚Äôll first isolate our ‚Äúproblem‚Äù characters.\n\ns_prob = \"œ± ‡§ú üòÄ\".encode(\"utf-8\")\nprint(s_prob)\nlist(s_prob)\n\nb'\\xcf\\xb1 \\xe0\\xa4\\x9c \\xf0\\x9f\\x98\\x80'\n\n\n[207, 177, 32, 224, 164, 156, 32, 240, 159, 152, 128]\n\n\nI know that a space character, ‚Äù ‚Äú, is represented by code point 32, so it seems that we have:\n\nœ± somehow equivalent to two code points 207, 177\n‡§ú somehow equivalent to three code points 224, 164, 156\nüòÄsomehow equivalent to the four code points 240, 159, 152, 128\n\nFor item 1 we could try:\n\nchr(207) + chr(177) # concatenate the two characters for the two code points\n\n'√è¬±'\n\n\nor\n\nchr(207 + 177) # find the character for the combined value of the two code points\n\n'∆Ä'\n\n\nNeither of those give the correct output. To get this to work I need to follow the UTF-8 guidelines for converting to Unicode code points (see table above). So for the Greek letter, let‚Äôs take a look at the byte values in binary and use the table to convert. \n\nfor b in s_prob[:2]:\n    print(b, hex(b), bin(b))\n\n207 0xcf 0b11001111\n177 0xb1 0b10110001\n\n\nThe \\(\\rm{\\textcolor{red}{0b}}\\) is Python‚Äôs designation for binary digit, and it is left out of the conversion table. So, instead of 207 and 177, we can deal with 11001111 and 10110001. Now we can follow the UTF-8 encoding. The \\(\\textcolor{blue}{110}\\) at the beginning of the first number is a code to indicate that the character requires two bytes (\\(\\textcolor{blue}{1110}\\) if it requires three bytes and \\(\\textcolor{blue}{11110}\\) if it requires four bytes). Any byte beginning with a \\(\\textcolor{blue}{10}\\) denotes to Python that it belongs to a sequence of either 2, 3, or 4 bytes, and that it is NOT the starting byte (the source of many UnicodeDecodeErrors, see below). Using this system, I get the following for œ±:\n\\[\n\\begin{align*}\n207 \\,\\, 177 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{110}\\, 01111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 110001 \\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,207\\,177}&= 01111 \\,\\,\\, 110001 \\\\\n&= 01111110001  \\\\\n\\rm{decimal\\,code\\, point\\, for\\,207\\,177}&= 0 \\times 2^{10} + 1 \\times 2^9 + 1 \\times 2^8 + 1 \\times 2^7 + 1 \\times 2^6 + 1 \\times 2^5 + 1 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 \\\\\n&= 0 + 512 + 256 + 128 + 64 + 32 + 16 + 0 + 0 + 0 + 1 \\\\\n&= 1009\n\\end{align*}\n\\]\nAnd just to validate that calculation:\n\nprint(int(0b01111110001))\nprint(2**9 + 2**8 + 2**7 + 2**6 + 2**5 + 2** 4 + 2**0)\n\n1009\n1009\n\n\nThis approach, then, provides a mechanism from going from stored binary digits to Unicode code points.\nSince I am a skeptical person, I want to see if this works for my other two ‚Äúproblem‚Äù characters. This time, however, I will leave out the direct conversion to decimal values. For these two characters, I have the following byte values:\n\nfor b in s_prob[3:]:\n    if b==32: # space character\n        print(\"space\")\n    else:\n        print(b, hex(b), bin(b))\n\n224 0xe0 0b11100000\n164 0xa4 0b10100100\n156 0x9c 0b10011100\nspace\n240 0xf0 0b11110000\n159 0x9f 0b10011111\n152 0x98 0b10011000\n128 0x80 0b10000000\n\n\nFor ‡§ú, I get the following:\n\\[\n\\begin{align*}\n224 \\,\\, 164\\,\\,156 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{1110}\\, 0000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 100100 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011100\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,224 \\,\\ 164\\,\\,156}&= 0000 \\,\\,\\, 100100 \\,\\,\\, 011100 \\\\\n&= 0000100100011100  \\\\\n\\end{align*}\n\\]\nwhich equals\n\nint(0b0000100100011100)\n\n2332\n\n\nAnd for üòÄ, I get:\n\\[\n\\begin{align*}\n240\\,\\,159 \\,\\, 152\\,\\,128 &= \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{11110}\\, 000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011111 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\, 011000 \\,\\,\\, \\rm{\\textcolor{red}{0b}}\\, \\textcolor{blue}{10}\\,000000\\\\\n\\\\\n\\rm{binary\\,code\\, point\\, for\\,240\\,\\,159 \\,\\, 152\\,\\,128}&= 000 \\,\\,\\, 011111 \\,\\,\\, 011000 \\,\\,\\, 000000 \\\\\n&= 000011111011000000000  \\\\\n\\end{align*}\n\\]\nwhich is\n\nint(0b000011111011000000000)\n\n128512\n\n\nSo, I can see that everything is working as it is supposed to.\nBefore wrapping up this part of my journey, I want to mention that UTF-8 is not the only encoding scheme. UTF-16 and UTF-32 also exist. However, since UTF-8 seems to be the dominant encoding scheme at the moment, I won‚Äôt venture into the lands of UTF-16 and UTF-32.\nAnd the last point I want to make is that the 2, 3, and 4 byte sequences used in UTF-8 are linked bytes, so all is well if I execute\n\nb'How are you? \\xf0\\x9f\\x98\\x80'.decode(\"utf-8\")\n\n'How are you? üòÄ'\n\n\nBut not so good if I execute this\n\nb'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 b'How are you? \\xf0 \\x9f\\x98\\x80'.decode(\"utf-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 13: invalid continuation byte\n\n\n\nThis is because the inserted space character is not a valid continuation byte according the the UTF-8 scheme.\nWell, that is it for the first part of my journey to dig into the details of BPE. Some aspects of encoding are still a bit murky, but I think I grasp enough at the moment to move on.\nDownload this notebook"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! This is where I will post Jupyter notebooks that I am using to understand various topics of interest. My hope is that they will help you too. s"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Filling in the Gaps",
    "section": "",
    "text": "Understanding Byte Pair Encoding: Part 2: Tokenization\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nMark Cassar\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Byte Pair Encoding: Part 1: Encodings\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nMark Cassar\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Byte Pair Encoding: Part 3: the Algorithm\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2024\n\n\nMark Cassar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/BPE_Part_2/BPE_Part_2.html",
    "href": "posts/BPE_Part_2/BPE_Part_2.html",
    "title": "Understanding Byte Pair Encoding: Part 2: Tokenization",
    "section": "",
    "text": "In my last post, I discussed encoding text, specifically using UTF-8. As I noted there, this encoding uses 1 to 4 bytes to represent all the characters in the Unicode system. This will be the byte part of byte pair encoding (BPE), which was introduced by Sennrich, Haddow, and Birch in a paper entitled Neural Machine Translation of Rare Words with Subword Units in 2015 (although the official publication was in 2016). It was proposed as a solution to the open vocabulary problem in machine translation. The method is an adaptation of a data compression technique developed by Philip Gage back in 1994.\nRare and out-of-vocabulary (OOV) words are a known issue when dealing with language, so the proposal was to tokenize text in a way that allows for subword units. In this way, rare or OOV words could be composed of the subunits. Sennrich et al.¬†showed that this approach gave better results than prior methods.\nNow, instead of encodings and bytes I want to come to grips with the terms vocabulary, out-of-vocabulary, and tokenize: what do they mean and why do I need them?\nI understand that large language models (LLMs) perform mathematical operations on their input to produce their output. So, it makes sense that they cannot process raw text. But, why not just use the numbers that come out of the UTF-8 encoding? The two main reasons against that approach that I can think of are:\n\nencodings are about characters, not meaning; for example, ‚Äòdog‚Äô and ‚Äòcanine‚Äô have similar meaning but very different encodings; likewise, ‚Äòact‚Äô and ‚Äòcat‚Äô would have similar encodings but have very different meanings; and\nin order to process any text we would need about 1 million numbers, which would drastically increase the computation required (compared to current methods)\n\n\n\nTo get to a better numeric representation of text, the usual starting point is tokenization. Tokenization is the process of breaking up text into smaller pieces called tokens, where a token is the smallest unit of text. Once it‚Äôs decided what a token should be, the set of unique tokens represents the vocabulary. The idea then is that given any raw text, I can represent it as a sequence of tokens from the vocabulary.\n\n\n\nThe process so far is:\n\ndefine what is meant by a token\ngather text that can be used as training data\nbreak the training data into tokens\ncreate a vocabulary from the unique set of tokens\nbreak up any new text into tokens from this vocabulary\n\nIt is a good time to try this out, so, I‚Äôll grab some text that is in the public domain:\n\nimport requests\n\nurl = \"https://www.gutenberg.org/cache/epub/2814/pg2814.txt\"\n\nresponse = requests.get(url)\ntext = response.text\ntext\ntext = text[1069:].replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \") # remove front matter and get rid of 'carriage' returns, ie. new lines\npar = text[:903] # grab the first paragraph\npar\n\n'THE SISTERS   There was no hope for him this time: it was the third stroke. Night after night I had passed the house (it was vacation time) and studied the lighted square of window: and night after night I had found it lighted in the same way, faintly and evenly. If he was dead, I thought, I would see the reflection of candles on the darkened blind for I knew that two candles must be set at the head of a corpse. He had often said to me: ‚ÄúI am not long for this world,‚Äù and I had thought his words idle. Now I knew they were true. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in my ears, like the word gnomon in the Euclid and the word simony in the Catechism. But now it sounded to me like the name of some maleficent and sinful being. It filled me with fear, and yet I longed to be nearer to it and to look upon its deadly work'\n\n\nI am going to ignore punctuation and define a token as a word:\n\nimport string \n\npunct = string.punctuation\n\npar_no_punct = ''.join(c for c in par if c not in string.punctuation)\npar_tokens = par_no_punct.split()  \nprint(len(par_tokens))\nprint(par_tokens)\n\n183\n['THE', 'SISTERS', 'There', 'was', 'no', 'hope', 'for', 'him', 'this', 'time', 'it', 'was', 'the', 'third', 'stroke', 'Night', 'after', 'night', 'I', 'had', 'passed', 'the', 'house', 'it', 'was', 'vacation', 'time', 'and', 'studied', 'the', 'lighted', 'square', 'of', 'window', 'and', 'night', 'after', 'night', 'I', 'had', 'found', 'it', 'lighted', 'in', 'the', 'same', 'way', 'faintly', 'and', 'evenly', 'If', 'he', 'was', 'dead', 'I', 'thought', 'I', 'would', 'see', 'the', 'reflection', 'of', 'candles', 'on', 'the', 'darkened', 'blind', 'for', 'I', 'knew', 'that', 'two', 'candles', 'must', 'be', 'set', 'at', 'the', 'head', 'of', 'a', 'corpse', 'He', 'had', 'often', 'said', 'to', 'me', '‚ÄúI', 'am', 'not', 'long', 'for', 'this', 'world‚Äù', 'and', 'I', 'had', 'thought', 'his', 'words', 'idle', 'Now', 'I', 'knew', 'they', 'were', 'true', 'Every', 'night', 'as', 'I', 'gazed', 'up', 'at', 'the', 'window', 'I', 'said', 'softly', 'to', 'myself', 'the', 'word', 'paralysis', 'It', 'had', 'always', 'sounded', 'strangely', 'in', 'my', 'ears', 'like', 'the', 'word', 'gnomon', 'in', 'the', 'Euclid', 'and', 'the', 'word', 'simony', 'in', 'the', 'Catechism', 'But', 'now', 'it', 'sounded', 'to', 'me', 'like', 'the', 'name', 'of', 'some', 'maleficent', 'and', 'sinful', 'being', 'It', 'filled', 'me', 'with', 'fear', 'and', 'yet', 'I', 'longed', 'to', 'be', 'nearer', 'to', 'it', 'and', 'to', 'look', 'upon', 'its', 'deadly', 'work']\n\n\nBased on defining a token at the word level, I have 183 tokens in my training data, which I took to be the first paragraph of the text. Now I would like to create the vocabulary, which is just the set of unique tokens:\n\nvocab = sorted(list(set(par_tokens)))\n\nprint(len(vocab))\nprint(vocab)\n\n109\n['But', 'Catechism', 'Euclid', 'Every', 'He', 'I', 'If', 'It', 'Night', 'Now', 'SISTERS', 'THE', 'There', 'a', 'after', 'always', 'am', 'and', 'as', 'at', 'be', 'being', 'blind', 'candles', 'corpse', 'darkened', 'dead', 'deadly', 'ears', 'evenly', 'faintly', 'fear', 'filled', 'for', 'found', 'gazed', 'gnomon', 'had', 'he', 'head', 'him', 'his', 'hope', 'house', 'idle', 'in', 'it', 'its', 'knew', 'lighted', 'like', 'long', 'longed', 'look', 'maleficent', 'me', 'must', 'my', 'myself', 'name', 'nearer', 'night', 'no', 'not', 'now', 'of', 'often', 'on', 'paralysis', 'passed', 'reflection', 'said', 'same', 'see', 'set', 'simony', 'sinful', 'softly', 'some', 'sounded', 'square', 'strangely', 'stroke', 'studied', 'that', 'the', 'they', 'third', 'this', 'thought', 'time', 'to', 'true', 'two', 'up', 'upon', 'vacation', 'was', 'way', 'were', 'window', 'with', 'word', 'words', 'work', 'world‚Äù', 'would', 'yet', '‚ÄúI']\n\n\nUsing this vocabulary, I can now try to tokenize any new text:\n\ndef check_tokens(tokens, vocab):\n    for token in tokens:\n        if token in vocab: \n            print(chr(10004), end=\" \") #f\"\\tToken = '{token}' is in the vocabulary\")\n        else:\n            print(f\"({chr(10006)} '{token}')\", end=\" \")\n\nnew_text = 'I like to be on vacation'\nnew_text_tokens = new_text.split()\n\nprint(f\"New text: {new_text}\")\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I', 'like', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî ‚úî ‚úî \n\n\n\n\n\nBut what if I wanted to tokenize a different sentence?\n\ndiff_text = \"I hate to be on vacation\"\ndiff_text_tokens = diff_text.split()\n\nprint(f\"Different text: {diff_text}\")\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab)\n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I', 'hate', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî (‚úñ 'hate') ‚úî ‚úî ‚úî ‚úî \n\n\nI see I have run into the OOV problem! (I am tempted to say that no one hates to be on vacation so there really is no need to tokenize this sentence!) The text I am currently trying to tokenize contains a token (in this case, a word) that does not exist in my vocabulary. What happened?\nLanguage is high-dimensional, that is, there are a lot of words. (And that is true just of English, not to mention all human languages.) Even though my example only considers a small amount of text as training data, to avoid this problem I would need to include all the text ever produced, which is not practical. Even that would only suffice until a new word was created and then I would be facing the OOV problem again. That is without even considering if that would be feasible computationally.\n\n\n\nAnother issue arises if I try to make sure the vocabulary size doesn‚Äôt get too big. To do this, I can limit the vocabulary to tokens that occur at or above some threshold frequency. To see how this works, I‚Äôll redo what I just did above, but this time I will only keep tokens in my vocabulary that occur more than once.\n\nt_freq = {}\nfor token in par_tokens:\n    t_freq[token] = t_freq.get(token, 0) + 1\n\ntokens_freq = [token for token in t_freq.keys() if t_freq[token] &gt; 1]\nvocab_freq = sorted(list(set(tokens_freq)))\nprint(len(vocab_freq))\nprint(vocab_freq)\n\n27\n['I', 'It', 'after', 'and', 'at', 'be', 'candles', 'for', 'had', 'in', 'it', 'knew', 'lighted', 'like', 'me', 'night', 'of', 'said', 'sounded', 'the', 'this', 'thought', 'time', 'to', 'was', 'window', 'word']\n\n\nApplying this new vocabulary to the same two sentences gives:\n\nprint(f\"New text: {new_text}\")\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab_freq)\nprint()\nprint()\nprint(f\"Different text: {diff_text}\")\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab_freq)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I', 'like', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî (‚úñ 'on') (‚úñ 'vacation') \n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I', 'hate', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî (‚úñ 'hate') ‚úî ‚úî (‚úñ 'on') (‚úñ 'vacation') \n\n\nI get the anticipated drop in vocabulary size from 109 to 27, however, I also see that now neither of the sentences can be properly tokenized. This is the rare word problem.\nOne way to deal with this is to always add a special token &lt;|unk|&gt; that can be used whenever I encounter a token that is not in the vocabulary.\nFor our current scenario, our sentences would be tokenized as follows:\n\nnew_text_tokens = [token if token in vocab_freq else '&lt;|unk|&gt;' for token in new_text.split()]\ndiff_text_tokens = [token if token in vocab_freq else '&lt;|unk|&gt;' for token in diff_text.split()]\n\nprint(new_text_tokens)\nprint(diff_text_tokens)\n\n['I', 'like', 'to', 'be', '&lt;|unk|&gt;', '&lt;|unk|&gt;']\n['I', '&lt;|unk|&gt;', 'to', 'be', '&lt;|unk|&gt;', '&lt;|unk|&gt;']\n\n\nThis removes both the rare and OOV problems, but it is not ideal. Now, any not in my vocabulary gets assigned the exact same token, &lt;|unk|&gt;, and the model will treat them all identically regardless of their original meaning.\n\n\n\nFor current LLMs computation happens at the token level. As far as I can tell, this means that if a given piece of raw text can be represented by fewer tokens, then it requires less computation; or, if I fix the computational resources, then I can process more raw text at a time (the so-called context window).\nIf that is the case, maybe going bigger than word level would be a good idea. I‚Äôll try sentences:\n\nimport re \n\nsent = re.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?|!)\\s', par)\nsent\n\nvocab_sent = []\nfor s in sent:\n    cleaned_s = ''.join(ch for ch in s if ch.isalnum() or ch.isspace())\n    vocab_sent.append(cleaned_s)\n\nvocab_sent\n\n['THE SISTERS   There was no hope for him this time it was the third stroke',\n 'Night after night I had passed the house it was vacation time and studied the lighted square of window and night after night I had found it lighted in the same way faintly and evenly',\n 'If he was dead I thought I would see the reflection of candles on the darkened blind for I knew that two candles must be set at the head of a corpse',\n 'He had often said to me I am not long for this world and I had thought his words idle',\n 'Now I knew they were true',\n 'Every night as I gazed up at the window I said softly to myself the word paralysis',\n 'It had always sounded strangely in my ears like the word gnomon in the Euclid and the word simony in the Catechism',\n 'But now it sounded to me like the name of some maleficent and sinful being',\n 'It filled me with fear and yet I longed to be nearer to it and to look upon its deadly work']\n\n\nNow, if I try to tokenize new text I‚Äôll see that it will be very unlikely that tokenization will occur properly:\n\nprint(f\"New text: {new_text}\")\nnew_text_tokens = [''.join(ch for ch in new_text if ch.isalnum() or ch.isspace())]\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab_sent)\nprint()\nprint()\nprint(f\"Different text: {diff_text}\")\ndiff_text_tokens = [''.join(ch for ch in diff_text if ch.isalnum() or ch.isspace())]\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab_sent)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I like to be on vacation']\n\nCheck if all tokens are in the vocabulary:\n(‚úñ 'I like to be on vacation') \n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I hate to be on vacation']\n\nCheck if all tokens are in the vocabulary:\n(‚úñ 'I hate to be on vacation') \n\n\nIf it were to work, then every sentence of raw text would only be one token, which would be computationally efficient. However, to get it to work, my training data would have to cover all possible sentences, which would make the vocabulary impractically large.\n\n\n\nWhat if, instead of making the tokens longer than words, I made them shorter? An obvious approach then would be to tokenize at the character level:\n\nvocab_ch = sorted(list(set(list(par))))\nvocab_ch = [ch for ch in vocab_ch if ch.isalnum() or ch.isspace()]\nprint(vocab_ch)\n\n[' ', 'B', 'C', 'E', 'H', 'I', 'N', 'R', 'S', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n\n\nLet‚Äôs see how this would work on my two sentences:\n\nprint(f\"New text: {new_text}\")\nnew_text_tokens = list(''.join(ch for ch in new_text if ch.isalnum() or ch.isspace()))\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab_ch)\nprint()\nprint()\nprint(f\"Different text: {diff_text}\")\ndiff_text_tokens = list(''.join(ch for ch in diff_text if ch.isalnum() or ch.isspace()))\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab_ch)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'o', 'n', ' ', 'v', 'a', 'c', 'a', 't', 'i', 'o', 'n']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî \n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I', ' ', 'h', 'a', 't', 'e', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'o', 'n', ' ', 'v', 'a', 'c', 'a', 't', 'i', 'o', 'n']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî \n\n\nIt works! And since all English words, by definition, can be constructed from the alphabet, I will never encounter a word that I cannot tokenize.\nBefore celebrating, I will do some comparisons using the text ‚ÄúI like to be on vacation‚Äù.\n\n\n\n\n\n\n\n\n\nTokenization Level\nNumber of Tokens\nRelative Vocabulary Size\nOOV/Rare Word Problems\n\n\n\n\ncharacter\n24\nsmall\nno\n\n\nword\n6\nmedium\nyes\n\n\nsentence\n1\nlarge\nyes\n\n\n\nI conclude that the ‚Äúno free lunch‚Äù axiom is correct. Removing some problems seems to potentially introduce problems somewhere else. For instance, tokenizing at the character level means there would be no such thing as OOV or rare word problems, but the cost is that for any give text there is a large increase in the number of tokens. So, if token count represents, on some level, computation then we need more computation for the same text then, say, word level tokenization. If I keep the computation level fixed, then character level tokenization will significantly reduce the context window, that is, how much text can be processed at the same time by the LLM.\nI think of byte pair encoding as a smart compromise between character and word level tokenization. And that is what I‚Äôll finally dig into next time."
  },
  {
    "objectID": "posts/BPE_Part_2/BPE_Part_2.html#understanding-byte-pair-encoding-part-2-tokenization",
    "href": "posts/BPE_Part_2/BPE_Part_2.html#understanding-byte-pair-encoding-part-2-tokenization",
    "title": "Understanding Byte Pair Encoding: Part 2: Tokenization",
    "section": "",
    "text": "In my last post, I discussed encoding text, specifically using UTF-8. As I noted there, this encoding uses 1 to 4 bytes to represent all the characters in the Unicode system. This will be the byte part of byte pair encoding (BPE), which was introduced by Sennrich, Haddow, and Birch in a paper entitled Neural Machine Translation of Rare Words with Subword Units in 2015 (although the official publication was in 2016). It was proposed as a solution to the open vocabulary problem in machine translation. The method is an adaptation of a data compression technique developed by Philip Gage back in 1994.\nRare and out-of-vocabulary (OOV) words are a known issue when dealing with language, so the proposal was to tokenize text in a way that allows for subword units. In this way, rare or OOV words could be composed of the subunits. Sennrich et al.¬†showed that this approach gave better results than prior methods.\nNow, instead of encodings and bytes I want to come to grips with the terms vocabulary, out-of-vocabulary, and tokenize: what do they mean and why do I need them?\nI understand that large language models (LLMs) perform mathematical operations on their input to produce their output. So, it makes sense that they cannot process raw text. But, why not just use the numbers that come out of the UTF-8 encoding? The two main reasons against that approach that I can think of are:\n\nencodings are about characters, not meaning; for example, ‚Äòdog‚Äô and ‚Äòcanine‚Äô have similar meaning but very different encodings; likewise, ‚Äòact‚Äô and ‚Äòcat‚Äô would have similar encodings but have very different meanings; and\nin order to process any text we would need about 1 million numbers, which would drastically increase the computation required (compared to current methods)\n\n\n\nTo get to a better numeric representation of text, the usual starting point is tokenization. Tokenization is the process of breaking up text into smaller pieces called tokens, where a token is the smallest unit of text. Once it‚Äôs decided what a token should be, the set of unique tokens represents the vocabulary. The idea then is that given any raw text, I can represent it as a sequence of tokens from the vocabulary.\n\n\n\nThe process so far is:\n\ndefine what is meant by a token\ngather text that can be used as training data\nbreak the training data into tokens\ncreate a vocabulary from the unique set of tokens\nbreak up any new text into tokens from this vocabulary\n\nIt is a good time to try this out, so, I‚Äôll grab some text that is in the public domain:\n\nimport requests\n\nurl = \"https://www.gutenberg.org/cache/epub/2814/pg2814.txt\"\n\nresponse = requests.get(url)\ntext = response.text\ntext\ntext = text[1069:].replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \") # remove front matter and get rid of 'carriage' returns, ie. new lines\npar = text[:903] # grab the first paragraph\npar\n\n'THE SISTERS   There was no hope for him this time: it was the third stroke. Night after night I had passed the house (it was vacation time) and studied the lighted square of window: and night after night I had found it lighted in the same way, faintly and evenly. If he was dead, I thought, I would see the reflection of candles on the darkened blind for I knew that two candles must be set at the head of a corpse. He had often said to me: ‚ÄúI am not long for this world,‚Äù and I had thought his words idle. Now I knew they were true. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in my ears, like the word gnomon in the Euclid and the word simony in the Catechism. But now it sounded to me like the name of some maleficent and sinful being. It filled me with fear, and yet I longed to be nearer to it and to look upon its deadly work'\n\n\nI am going to ignore punctuation and define a token as a word:\n\nimport string \n\npunct = string.punctuation\n\npar_no_punct = ''.join(c for c in par if c not in string.punctuation)\npar_tokens = par_no_punct.split()  \nprint(len(par_tokens))\nprint(par_tokens)\n\n183\n['THE', 'SISTERS', 'There', 'was', 'no', 'hope', 'for', 'him', 'this', 'time', 'it', 'was', 'the', 'third', 'stroke', 'Night', 'after', 'night', 'I', 'had', 'passed', 'the', 'house', 'it', 'was', 'vacation', 'time', 'and', 'studied', 'the', 'lighted', 'square', 'of', 'window', 'and', 'night', 'after', 'night', 'I', 'had', 'found', 'it', 'lighted', 'in', 'the', 'same', 'way', 'faintly', 'and', 'evenly', 'If', 'he', 'was', 'dead', 'I', 'thought', 'I', 'would', 'see', 'the', 'reflection', 'of', 'candles', 'on', 'the', 'darkened', 'blind', 'for', 'I', 'knew', 'that', 'two', 'candles', 'must', 'be', 'set', 'at', 'the', 'head', 'of', 'a', 'corpse', 'He', 'had', 'often', 'said', 'to', 'me', '‚ÄúI', 'am', 'not', 'long', 'for', 'this', 'world‚Äù', 'and', 'I', 'had', 'thought', 'his', 'words', 'idle', 'Now', 'I', 'knew', 'they', 'were', 'true', 'Every', 'night', 'as', 'I', 'gazed', 'up', 'at', 'the', 'window', 'I', 'said', 'softly', 'to', 'myself', 'the', 'word', 'paralysis', 'It', 'had', 'always', 'sounded', 'strangely', 'in', 'my', 'ears', 'like', 'the', 'word', 'gnomon', 'in', 'the', 'Euclid', 'and', 'the', 'word', 'simony', 'in', 'the', 'Catechism', 'But', 'now', 'it', 'sounded', 'to', 'me', 'like', 'the', 'name', 'of', 'some', 'maleficent', 'and', 'sinful', 'being', 'It', 'filled', 'me', 'with', 'fear', 'and', 'yet', 'I', 'longed', 'to', 'be', 'nearer', 'to', 'it', 'and', 'to', 'look', 'upon', 'its', 'deadly', 'work']\n\n\nBased on defining a token at the word level, I have 183 tokens in my training data, which I took to be the first paragraph of the text. Now I would like to create the vocabulary, which is just the set of unique tokens:\n\nvocab = sorted(list(set(par_tokens)))\n\nprint(len(vocab))\nprint(vocab)\n\n109\n['But', 'Catechism', 'Euclid', 'Every', 'He', 'I', 'If', 'It', 'Night', 'Now', 'SISTERS', 'THE', 'There', 'a', 'after', 'always', 'am', 'and', 'as', 'at', 'be', 'being', 'blind', 'candles', 'corpse', 'darkened', 'dead', 'deadly', 'ears', 'evenly', 'faintly', 'fear', 'filled', 'for', 'found', 'gazed', 'gnomon', 'had', 'he', 'head', 'him', 'his', 'hope', 'house', 'idle', 'in', 'it', 'its', 'knew', 'lighted', 'like', 'long', 'longed', 'look', 'maleficent', 'me', 'must', 'my', 'myself', 'name', 'nearer', 'night', 'no', 'not', 'now', 'of', 'often', 'on', 'paralysis', 'passed', 'reflection', 'said', 'same', 'see', 'set', 'simony', 'sinful', 'softly', 'some', 'sounded', 'square', 'strangely', 'stroke', 'studied', 'that', 'the', 'they', 'third', 'this', 'thought', 'time', 'to', 'true', 'two', 'up', 'upon', 'vacation', 'was', 'way', 'were', 'window', 'with', 'word', 'words', 'work', 'world‚Äù', 'would', 'yet', '‚ÄúI']\n\n\nUsing this vocabulary, I can now try to tokenize any new text:\n\ndef check_tokens(tokens, vocab):\n    for token in tokens:\n        if token in vocab: \n            print(chr(10004), end=\" \") #f\"\\tToken = '{token}' is in the vocabulary\")\n        else:\n            print(f\"({chr(10006)} '{token}')\", end=\" \")\n\nnew_text = 'I like to be on vacation'\nnew_text_tokens = new_text.split()\n\nprint(f\"New text: {new_text}\")\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I', 'like', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî ‚úî ‚úî \n\n\n\n\n\nBut what if I wanted to tokenize a different sentence?\n\ndiff_text = \"I hate to be on vacation\"\ndiff_text_tokens = diff_text.split()\n\nprint(f\"Different text: {diff_text}\")\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab)\n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I', 'hate', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî (‚úñ 'hate') ‚úî ‚úî ‚úî ‚úî \n\n\nI see I have run into the OOV problem! (I am tempted to say that no one hates to be on vacation so there really is no need to tokenize this sentence!) The text I am currently trying to tokenize contains a token (in this case, a word) that does not exist in my vocabulary. What happened?\nLanguage is high-dimensional, that is, there are a lot of words. (And that is true just of English, not to mention all human languages.) Even though my example only considers a small amount of text as training data, to avoid this problem I would need to include all the text ever produced, which is not practical. Even that would only suffice until a new word was created and then I would be facing the OOV problem again. That is without even considering if that would be feasible computationally.\n\n\n\nAnother issue arises if I try to make sure the vocabulary size doesn‚Äôt get too big. To do this, I can limit the vocabulary to tokens that occur at or above some threshold frequency. To see how this works, I‚Äôll redo what I just did above, but this time I will only keep tokens in my vocabulary that occur more than once.\n\nt_freq = {}\nfor token in par_tokens:\n    t_freq[token] = t_freq.get(token, 0) + 1\n\ntokens_freq = [token for token in t_freq.keys() if t_freq[token] &gt; 1]\nvocab_freq = sorted(list(set(tokens_freq)))\nprint(len(vocab_freq))\nprint(vocab_freq)\n\n27\n['I', 'It', 'after', 'and', 'at', 'be', 'candles', 'for', 'had', 'in', 'it', 'knew', 'lighted', 'like', 'me', 'night', 'of', 'said', 'sounded', 'the', 'this', 'thought', 'time', 'to', 'was', 'window', 'word']\n\n\nApplying this new vocabulary to the same two sentences gives:\n\nprint(f\"New text: {new_text}\")\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab_freq)\nprint()\nprint()\nprint(f\"Different text: {diff_text}\")\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab_freq)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I', 'like', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî (‚úñ 'on') (‚úñ 'vacation') \n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I', 'hate', 'to', 'be', 'on', 'vacation']\n\nCheck if all tokens are in the vocabulary:\n‚úî (‚úñ 'hate') ‚úî ‚úî (‚úñ 'on') (‚úñ 'vacation') \n\n\nI get the anticipated drop in vocabulary size from 109 to 27, however, I also see that now neither of the sentences can be properly tokenized. This is the rare word problem.\nOne way to deal with this is to always add a special token &lt;|unk|&gt; that can be used whenever I encounter a token that is not in the vocabulary.\nFor our current scenario, our sentences would be tokenized as follows:\n\nnew_text_tokens = [token if token in vocab_freq else '&lt;|unk|&gt;' for token in new_text.split()]\ndiff_text_tokens = [token if token in vocab_freq else '&lt;|unk|&gt;' for token in diff_text.split()]\n\nprint(new_text_tokens)\nprint(diff_text_tokens)\n\n['I', 'like', 'to', 'be', '&lt;|unk|&gt;', '&lt;|unk|&gt;']\n['I', '&lt;|unk|&gt;', 'to', 'be', '&lt;|unk|&gt;', '&lt;|unk|&gt;']\n\n\nThis removes both the rare and OOV problems, but it is not ideal. Now, any not in my vocabulary gets assigned the exact same token, &lt;|unk|&gt;, and the model will treat them all identically regardless of their original meaning.\n\n\n\nFor current LLMs computation happens at the token level. As far as I can tell, this means that if a given piece of raw text can be represented by fewer tokens, then it requires less computation; or, if I fix the computational resources, then I can process more raw text at a time (the so-called context window).\nIf that is the case, maybe going bigger than word level would be a good idea. I‚Äôll try sentences:\n\nimport re \n\nsent = re.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?|!)\\s', par)\nsent\n\nvocab_sent = []\nfor s in sent:\n    cleaned_s = ''.join(ch for ch in s if ch.isalnum() or ch.isspace())\n    vocab_sent.append(cleaned_s)\n\nvocab_sent\n\n['THE SISTERS   There was no hope for him this time it was the third stroke',\n 'Night after night I had passed the house it was vacation time and studied the lighted square of window and night after night I had found it lighted in the same way faintly and evenly',\n 'If he was dead I thought I would see the reflection of candles on the darkened blind for I knew that two candles must be set at the head of a corpse',\n 'He had often said to me I am not long for this world and I had thought his words idle',\n 'Now I knew they were true',\n 'Every night as I gazed up at the window I said softly to myself the word paralysis',\n 'It had always sounded strangely in my ears like the word gnomon in the Euclid and the word simony in the Catechism',\n 'But now it sounded to me like the name of some maleficent and sinful being',\n 'It filled me with fear and yet I longed to be nearer to it and to look upon its deadly work']\n\n\nNow, if I try to tokenize new text I‚Äôll see that it will be very unlikely that tokenization will occur properly:\n\nprint(f\"New text: {new_text}\")\nnew_text_tokens = [''.join(ch for ch in new_text if ch.isalnum() or ch.isspace())]\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab_sent)\nprint()\nprint()\nprint(f\"Different text: {diff_text}\")\ndiff_text_tokens = [''.join(ch for ch in diff_text if ch.isalnum() or ch.isspace())]\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab_sent)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I like to be on vacation']\n\nCheck if all tokens are in the vocabulary:\n(‚úñ 'I like to be on vacation') \n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I hate to be on vacation']\n\nCheck if all tokens are in the vocabulary:\n(‚úñ 'I hate to be on vacation') \n\n\nIf it were to work, then every sentence of raw text would only be one token, which would be computationally efficient. However, to get it to work, my training data would have to cover all possible sentences, which would make the vocabulary impractically large.\n\n\n\nWhat if, instead of making the tokens longer than words, I made them shorter? An obvious approach then would be to tokenize at the character level:\n\nvocab_ch = sorted(list(set(list(par))))\nvocab_ch = [ch for ch in vocab_ch if ch.isalnum() or ch.isspace()]\nprint(vocab_ch)\n\n[' ', 'B', 'C', 'E', 'H', 'I', 'N', 'R', 'S', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n\n\nLet‚Äôs see how this would work on my two sentences:\n\nprint(f\"New text: {new_text}\")\nnew_text_tokens = list(''.join(ch for ch in new_text if ch.isalnum() or ch.isspace()))\nprint(f\"Possible tokenization: {new_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(new_text_tokens, vocab_ch)\nprint()\nprint()\nprint(f\"Different text: {diff_text}\")\ndiff_text_tokens = list(''.join(ch for ch in diff_text if ch.isalnum() or ch.isspace()))\nprint(f\"Possible tokenization: {diff_text_tokens}\")\nprint()\nprint(\"Check if all tokens are in the vocabulary:\")\ncheck_tokens(diff_text_tokens, vocab_ch)\n\nNew text: I like to be on vacation\nPossible tokenization: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'o', 'n', ' ', 'v', 'a', 'c', 'a', 't', 'i', 'o', 'n']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî \n\nDifferent text: I hate to be on vacation\nPossible tokenization: ['I', ' ', 'h', 'a', 't', 'e', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'o', 'n', ' ', 'v', 'a', 'c', 'a', 't', 'i', 'o', 'n']\n\nCheck if all tokens are in the vocabulary:\n‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî ‚úî \n\n\nIt works! And since all English words, by definition, can be constructed from the alphabet, I will never encounter a word that I cannot tokenize.\nBefore celebrating, I will do some comparisons using the text ‚ÄúI like to be on vacation‚Äù.\n\n\n\n\n\n\n\n\n\nTokenization Level\nNumber of Tokens\nRelative Vocabulary Size\nOOV/Rare Word Problems\n\n\n\n\ncharacter\n24\nsmall\nno\n\n\nword\n6\nmedium\nyes\n\n\nsentence\n1\nlarge\nyes\n\n\n\nI conclude that the ‚Äúno free lunch‚Äù axiom is correct. Removing some problems seems to potentially introduce problems somewhere else. For instance, tokenizing at the character level means there would be no such thing as OOV or rare word problems, but the cost is that for any give text there is a large increase in the number of tokens. So, if token count represents, on some level, computation then we need more computation for the same text then, say, word level tokenization. If I keep the computation level fixed, then character level tokenization will significantly reduce the context window, that is, how much text can be processed at the same time by the LLM.\nI think of byte pair encoding as a smart compromise between character and word level tokenization. And that is what I‚Äôll finally dig into next time."
  }
]